{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oeXZ8PK2N2af",
    "outputId": "375887b5-91de-422e-b437-4bbb5342ab10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperas in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.4)\n",
      "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.5.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.3.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.5.1)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (6.0.0)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.4.2)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.8.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.2)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.5.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->hyperas) (5.2.4)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (5.5.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (2.0.9)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (4.5.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.4.2)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt->hyperas) (4.4.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (17.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.5.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (4.7.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.7.5)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->hyperas) (41.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
      "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.16.4)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.8.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperas\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDHzgvf8Yckz"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkehXSi2jXxZ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tseGWviO6xGD",
    "outputId": "426271e1-60cb-49fe-87de-f99a8ecb6e9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "d_mlByJ21PH1",
    "outputId": "2405a84a-0749-4713-90b3-b678555c4fe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ee4NtKOQg1jF"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "  '''\n",
    "  Data providing function:\n",
    "  This function is separated from model() so that hyperopt\n",
    "  won't reload data for each evaluation run.\n",
    "  '''\n",
    "  pickle_in = open(\"/content/drive/My Drive/HAR/X_train.pickle\",\"rb\")\n",
    "  X_train = pickle.load(pickle_in)\n",
    "  pickle_in.close()\n",
    "\n",
    "  pickle_in = open(\"/content/drive/My Drive/HAR/X_test.pickle\",\"rb\")\n",
    "  X_test = pickle.load(pickle_in)\n",
    "  pickle_in.close()\n",
    "\n",
    "  pickle_in = open(\"/content/drive/My Drive/HAR/Y_train.pickle\",\"rb\")\n",
    "  Y_train = pickle.load(pickle_in)\n",
    "  pickle_in.close()\n",
    "\n",
    "  pickle_in = open(\"/content/drive/My Drive/HAR/Y_test.pickle\",\"rb\")\n",
    "  Y_test = pickle.load(pickle_in)\n",
    "  pickle_in.close()\n",
    "  return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0LDjLDyTaI8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjYR0Vjgi7II"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "N2OL2rrGToS3",
    "outputId": "4f6bf059-b71c-43be-829f-68f9cb40c8b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOetoSVUjjnC"
   },
   "outputs": [],
   "source": [
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "CcWu-St5kOit",
    "outputId": "2a415ee6-d081-4cd3-af51-d24ed3a6d9d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▎                               | 10kB 20.9MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 25.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 29.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 40kB 32.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 51kB 35.7MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 61kB 38.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 71kB 38.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 81kB 39.6MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 92kB 41.7MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 102kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 112kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 122kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 133kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 143kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 153kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▎                          | 163kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 174kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 184kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 194kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 204kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 215kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 225kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 235kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 245kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 256kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 266kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 276kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 286kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 296kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 307kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 317kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 327kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 337kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 348kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 358kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 368kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 378kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 389kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 399kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▎                  | 409kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 419kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 430kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▎                 | 440kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 450kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 460kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 471kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 481kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 491kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 501kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 512kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 522kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 532kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 542kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 552kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 563kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 573kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 583kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 593kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 604kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 614kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 624kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 634kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 645kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 655kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 665kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 675kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 686kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 696kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 706kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 716kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 727kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 737kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 747kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 757kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 768kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 778kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 788kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 798kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 808kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 819kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 829kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 839kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 849kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 860kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 870kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▌   | 880kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 890kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 901kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 911kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 921kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 931kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 942kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 952kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 962kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 972kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 983kB 43.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 993kB 43.2MB/s \n",
      "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Copy/download the file\n",
    "fid = drive.ListFile({'q':\"title='HAR_LSTM_OPT.ipynb'\"}).GetList()[0]['id']\n",
    "f = drive.CreateFile({'id': fid})\n",
    "f.GetContentFile('HAR_LSTM_OPT.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "dXNNqv2jjEJJ",
    "outputId": "714af4e4-a4b2-410d-904c-494f696cd38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VSQE6EOiigwL",
    "outputId": "47354508-bdfc-4f11-d456-941aa3c91aa2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zjh0CXySJKQr"
   },
   "source": [
    "# Hyperperameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf6v-ZqoNcPX"
   },
   "outputs": [],
   "source": [
    "# Initiliazing the sequential model\n",
    "def model(X_train, Y_train, X_test, Y_test):\n",
    "    from keras.models import Sequential\n",
    "    import keras\n",
    "    from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
    "    from keras.layers.core import Dense, Dropout, Flatten\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    \n",
    "    timesteps = 128\n",
    "    input_dim = 9\n",
    "    n_classes = 6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    model.add(Conv1D(filters = {{choice([64,128, 256, 512, 1024])}}, kernel_size={{choice([2,3,4,5])}},\n",
    "                     activation={{choice(['relu', 'sigmoid'])}},padding = 'same', input_shape = (timesteps,input_dim)))\n",
    "    model.add(MaxPooling1D(pool_size={{choice([2,3])}}))\n",
    "    # model.add(TimeDistributed(BatchNormalization()))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Conv1D(filters = {{choice([64,128, 256, 512, 1024])}}, kernel_size={{choice([2,3,4,5])}},\n",
    "                     activation={{choice(['relu', 'sigmoid'])}},padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size={{choice([2,3])}}))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Conv1D(filters = {{choice([64,128, 256, 512, 1024])}}, kernel_size={{choice([2,3,4,5])}},\n",
    "         activation={{choice(['relu', 'sigmoid'])}},padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    if {{choice(['three','four'])}} == 'four':\n",
    "        model.add(Conv1D(filters = {{choice([64,128, 256, 512, 1024])}}, kernel_size={{choice([2,3,4,5])}},\n",
    "                     activation={{choice(['relu', 'sigmoid'])}},padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "        # model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(Dropout({{uniform(0, 1)}}))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense({{choice([64,128, 256, 512, 1024])}}, activation={{choice(['softmax', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([64,128, 256, 512, 1024])}}, activation={{choice(['softmax', 'sigmoid'])}}))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([64,128, 256, 512, 1024])}}, activation={{choice(['softmax', 'sigmoid'])}}))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(n_classes, activation={{choice(['softmax', 'sigmoid'])}}))\n",
    "\n",
    "    adam = keras.optimizers.Adam(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "    sgd = keras.optimizers.SGD(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "\n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size={{choice([16,64,128,256])}},\n",
    "              nb_epoch=30,\n",
    "              verbose=2,\n",
    "              validation_data=(X_test, Y_test))\n",
    "    score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YPG6dKXOS8q7",
    "outputId": "cce51641-d7af-4b6c-f6f3-c91ae15e41d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from google.colab import drive\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.auth import GoogleAuth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from pydrive.drive import GoogleDrive\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from google.colab import auth\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from oauth2client.client import GoogleCredentials\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.normalization import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.normalization import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.normalization import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'filters': hp.choice('filters', [64,128, 256, 512, 1024]),\n",
      "        'kernel_size': hp.choice('kernel_size', [2,3,4,5]),\n",
      "        'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
      "        'pool_size': hp.choice('pool_size', [2,3]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'filters_1': hp.choice('filters_1', [64,128, 256, 512, 1024]),\n",
      "        'kernel_size_1': hp.choice('kernel_size_1', [2,3,4,5]),\n",
      "        'activation_1': hp.choice('activation_1', ['relu', 'sigmoid']),\n",
      "        'pool_size_1': hp.choice('pool_size_1', [2,3]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'filters_2': hp.choice('filters_2', [64,128, 256, 512, 1024]),\n",
      "        'kernel_size_2': hp.choice('kernel_size_2', [2,3,4,5]),\n",
      "        'activation_2': hp.choice('activation_2', ['relu', 'sigmoid']),\n",
      "        'pool_size_2': hp.choice('pool_size_2', [1,2,3]),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'Dropout_3': hp.choice('Dropout_3', ['three','four']),\n",
      "        'filters_3': hp.choice('filters_3', [64,128, 256, 512, 1024]),\n",
      "        'kernel_size_3': hp.choice('kernel_size_3', [2,3,4,5]),\n",
      "        'activation_3': hp.choice('activation_3', ['relu', 'sigmoid']),\n",
      "        'pool_size_3': hp.choice('pool_size_3', [1,2,3]),\n",
      "        'Dropout_4': hp.uniform('Dropout_4', 0, 1),\n",
      "        'filters_4': hp.choice('filters_4', [64,128, 256, 512, 1024]),\n",
      "        'activation_4': hp.choice('activation_4', ['softmax', 'sigmoid']),\n",
      "        'Dropout_5': hp.uniform('Dropout_5', 0, 1),\n",
      "        'filters_5': hp.choice('filters_5', [64,128, 256, 512, 1024]),\n",
      "        'activation_5': hp.choice('activation_5', ['softmax', 'sigmoid']),\n",
      "        'Dropout_6': hp.uniform('Dropout_6', 0, 1),\n",
      "        'filters_6': hp.choice('filters_6', [64,128, 256, 512, 1024]),\n",
      "        'activation_6': hp.choice('activation_6', ['softmax', 'sigmoid']),\n",
      "        'Dropout_7': hp.uniform('Dropout_7', 0, 1),\n",
      "        'activation_7': hp.choice('activation_7', ['softmax', 'sigmoid']),\n",
      "        'lr': hp.choice('lr', [10**-4,10**-3, 10**-2, 10**-1]),\n",
      "        'lr_1': hp.choice('lr_1', [10**-4,10**-3, 10**-2, 10**-1]),\n",
      "        'lr_2': hp.choice('lr_2', [10**-4,10**-3, 10**-2, 10**-1]),\n",
      "        'choiceval': hp.choice('choiceval', ['adam', 'sgd', 'rmsprop']),\n",
      "        'batch_size': hp.choice('batch_size', [16,64,128,256]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: '''\n",
      "  3: Data providing function:\n",
      "  4: This function is separated from model() so that hyperopt\n",
      "  5: won't reload data for each evaluation run.\n",
      "  6: '''\n",
      "  7: pickle_in = open(\"/content/drive/My Drive/HAR/X_train.pickle\",\"rb\")\n",
      "  8: X_train = pickle.load(pickle_in)\n",
      "  9: pickle_in.close()\n",
      " 10: \n",
      " 11: pickle_in = open(\"/content/drive/My Drive/HAR/X_test.pickle\",\"rb\")\n",
      " 12: X_test = pickle.load(pickle_in)\n",
      " 13: pickle_in.close()\n",
      " 14: \n",
      " 15: pickle_in = open(\"/content/drive/My Drive/HAR/Y_train.pickle\",\"rb\")\n",
      " 16: Y_train = pickle.load(pickle_in)\n",
      " 17: pickle_in.close()\n",
      " 18: \n",
      " 19: pickle_in = open(\"/content/drive/My Drive/HAR/Y_test.pickle\",\"rb\")\n",
      " 20: Y_test = pickle.load(pickle_in)\n",
      " 21: pickle_in.close()\n",
      " 22: \n",
      " 23: \n",
      " 24: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \n",
      "   4:     timesteps = 128\n",
      "   5:     input_dim = 9\n",
      "   6:     n_classes = 6\n",
      "   7:     model = Sequential()\n",
      "   8:     # Configuring the parameters\n",
      "   9:     model.add(Conv1D(filters = space['filters'], kernel_size=space['kernel_size'],\n",
      "  10:                      activation=space['activation'],padding = 'same', input_shape = (timesteps,input_dim)))\n",
      "  11:     model.add(MaxPooling1D(pool_size=space['pool_size']))\n",
      "  12:     # model.add(TimeDistributed(BatchNormalization()))\n",
      "  13:     model.add(Dropout(space['Dropout']))\n",
      "  14: \n",
      "  15:     model.add(Conv1D(filters = space['filters_1'], kernel_size=space['kernel_size_1'],\n",
      "  16:                      activation=space['activation_1'],padding = 'same'))\n",
      "  17:     model.add(MaxPooling1D(pool_size=space['pool_size_1']))\n",
      "  18:     model.add(BatchNormalization())\n",
      "  19:     model.add(Dropout(space['Dropout_1']))\n",
      "  20:     \n",
      "  21:     model.add(Conv1D(filters = space['filters_2'], kernel_size=space['kernel_size_2'],\n",
      "  22:          activation=space['activation_2'],padding = 'same'))\n",
      "  23: #         model.add(MaxPooling1D(pool_size=space['pool_size_2']))\n",
      "  24:     model.add(BatchNormalization())\n",
      "  25:     model.add(Dropout(space['Dropout_2']))\n",
      "  26: \n",
      "  27:     if space['Dropout_3'] == 'four':\n",
      "  28:         model.add(Conv1D(filters = space['filters_3'], kernel_size=space['kernel_size_3'],\n",
      "  29:                      activation=space['activation_3'],padding = 'same'))\n",
      "  30: #         model.add(MaxPooling1D(pool_size=space['pool_size_3']))\n",
      "  31:         # model.add(TimeDistributed(BatchNormalization()))\n",
      "  32:         model.add(Dropout(space['Dropout_4']))\n",
      "  33: \n",
      "  34:     model.add(Flatten())\n",
      "  35:     \n",
      "  36:     model.add(Dense(space['filters_4'], activation=space['activation_4']))\n",
      "  37:     model.add(Dropout(space['Dropout_5']))\n",
      "  38:     model.add(Dense(space['filters_5'], activation=space['activation_5']))\n",
      "  39:     model.add(Dropout(space['Dropout_6']))\n",
      "  40:     model.add(Dense(space['filters_6'], activation=space['activation_6']))\n",
      "  41:     # Adding a dense output layer with sigmoid activation\n",
      "  42:     model.add(Dropout(space['Dropout_7']))\n",
      "  43:     model.add(Dense(n_classes, activation=space['activation_7']))\n",
      "  44: \n",
      "  45:     adam = keras.optimizers.Adam(lr=space['lr'])\n",
      "  46:     rmsprop = keras.optimizers.RMSprop(lr=space['lr_1'])\n",
      "  47:     sgd = keras.optimizers.SGD(lr=space['lr_2'])\n",
      "  48: \n",
      "  49:     choiceval = space['choiceval']\n",
      "  50:     if choiceval == 'adam':\n",
      "  51:         optim = adam\n",
      "  52:     elif choiceval == 'rmsprop':\n",
      "  53:         optim = rmsprop\n",
      "  54:     else:\n",
      "  55:         optim = sgd\n",
      "  56: \n",
      "  57:     model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=optim)\n",
      "  58:     model.fit(X_train, Y_train,\n",
      "  59:               batch_size=space['batch_size'],\n",
      "  60:               nb_epoch=30,\n",
      "  61:               verbose=2,\n",
      "  62:               validation_data=(X_test, Y_test))\n",
      "  63:     score, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
      "  64:     print('Test accuracy:', acc)\n",
      "  65:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  66: \n",
      "  0%|          | 0/30 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "\n",
      "W0626 12:56:47.354541 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0626 12:56:47.384180 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0626 12:56:47.391540 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0626 12:56:47.425362 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0626 12:56:47.429435 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0626 12:56:47.437600 140419024013184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0626 12:56:47.575175 140419024013184 nn_ops.py:4224] Large dropout rate: 0.908513 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0626 12:56:47.728194 140419024013184 nn_ops.py:4224] Large dropout rate: 0.892863 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0626 12:56:47.767098 140419024013184 nn_ops.py:4224] Large dropout rate: 0.671461 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0626 12:56:47.994699 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0626 12:56:48.003738 140419024013184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n",
      "W0626 12:56:48.113967 140419024013184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 10s - loss: 1.7997 - acc: 0.1831 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 2s - loss: 1.7880 - acc: 0.1868 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 2s - loss: 1.7866 - acc: 0.1861 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 2s - loss: 1.7870 - acc: 0.1853 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 2s - loss: 1.7866 - acc: 0.1878 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 2s - loss: 1.7860 - acc: 0.1902 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 2s - loss: 1.7859 - acc: 0.2001 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 2s - loss: 1.7857 - acc: 0.1938 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 2s - loss: 1.7865 - acc: 0.1926 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 2s - loss: 1.7862 - acc: 0.1891 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 2s - loss: 1.7865 - acc: 0.1953 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 2s - loss: 1.7857 - acc: 0.1887 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 2s - loss: 1.7854 - acc: 0.1899 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 2s - loss: 1.7849 - acc: 0.1906 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 2s - loss: 1.7862 - acc: 0.1906 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 2s - loss: 1.7857 - acc: 0.1979 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 2s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 2s - loss: 1.7857 - acc: 0.1921 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 2s - loss: 1.7854 - acc: 0.1892 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 2s - loss: 1.7854 - acc: 0.1892 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 2s - loss: 1.7849 - acc: 0.1889 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 2s - loss: 1.7854 - acc: 0.1938 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 2s - loss: 1.7848 - acc: 0.1908 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 2s - loss: 1.7850 - acc: 0.1916 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 2s - loss: 1.7849 - acc: 0.1925 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 2s - loss: 1.7851 - acc: 0.1881 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 2s - loss: 1.7851 - acc: 0.1918 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 2s - loss: 1.7847 - acc: 0.1873 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 2s - loss: 1.7845 - acc: 0.1921 - val_loss: 1.7903 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 2s - loss: 1.7852 - acc: 0.1908 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      "  3%|▎         | 1/30 [01:09<33:31, 69.35s/it, best loss: -0.18221920597217509]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 12:57:56.738947 140419024013184 nn_ops.py:4224] Large dropout rate: 0.556238 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0626 12:57:56.863257 140419024013184 nn_ops.py:4224] Large dropout rate: 0.877458 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 10s - loss: 1.7898 - acc: 0.1857 - val_loss: 1.7898 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 6s - loss: 1.7875 - acc: 0.1843 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 6s - loss: 1.7862 - acc: 0.1851 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.7854 - acc: 0.1881 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 6s - loss: 1.7848 - acc: 0.1908 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 6s - loss: 1.7844 - acc: 0.1916 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 6s - loss: 1.7843 - acc: 0.1930 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 6s - loss: 1.7841 - acc: 0.1923 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 6s - loss: 1.7841 - acc: 0.1903 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 6s - loss: 1.7841 - acc: 0.1908 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1916 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1918 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1926 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1925 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1912 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 6s - loss: 1.7841 - acc: 0.1922 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 6s - loss: 1.7838 - acc: 0.1914 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1919 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1912 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1912 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1880 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 6s - loss: 1.7842 - acc: 0.1922 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1923 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 6s - loss: 1.7838 - acc: 0.1907 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1912 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 6s - loss: 1.7840 - acc: 0.1926 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1902 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 6s - loss: 1.7839 - acc: 0.1916 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 6s - loss: 1.7842 - acc: 0.1911 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      "  7%|▋         | 2/30 [04:21<49:35, 106.28s/it, best loss: -0.18221920597217509]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 8s - loss: 1.8042 - acc: 0.1829 - val_loss: 1.8323 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 6s - loss: 1.7983 - acc: 0.1906 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 6s - loss: 1.7963 - acc: 0.1912 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.7961 - acc: 0.1915 - val_loss: 1.7918 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 6s - loss: 1.8017 - acc: 0.1789 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 6s - loss: 1.7990 - acc: 0.1872 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 6s - loss: 1.8015 - acc: 0.1809 - val_loss: 1.7889 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 6s - loss: 1.8024 - acc: 0.1923 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 6s - loss: 1.7987 - acc: 0.1907 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 6s - loss: 1.7992 - acc: 0.1827 - val_loss: 1.7901 - val_acc: 0.1666\n",
      "\n",
      "Epoch 11/30\n",
      " - 6s - loss: 1.7993 - acc: 0.1827 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 6s - loss: 1.8002 - acc: 0.1840 - val_loss: 1.7903 - val_acc: 0.1805\n",
      "\n",
      "Epoch 13/30\n",
      " - 6s - loss: 1.7993 - acc: 0.1872 - val_loss: 1.7900 - val_acc: 0.1666\n",
      "\n",
      "Epoch 14/30\n",
      " - 6s - loss: 1.7999 - acc: 0.1892 - val_loss: 1.7903 - val_acc: 0.1805\n",
      "\n",
      "Epoch 15/30\n",
      " - 6s - loss: 1.8027 - acc: 0.1877 - val_loss: 1.7913 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 6s - loss: 1.8032 - acc: 0.1846 - val_loss: 1.7916 - val_acc: 0.1666\n",
      "\n",
      "Epoch 17/30\n",
      " - 6s - loss: 1.8038 - acc: 0.1869 - val_loss: 1.7915 - val_acc: 0.1805\n",
      "\n",
      "Epoch 18/30\n",
      " - 6s - loss: 1.8031 - acc: 0.1887 - val_loss: 1.7915 - val_acc: 0.1805\n",
      "\n",
      "Epoch 19/30\n",
      " - 6s - loss: 1.8013 - acc: 0.1831 - val_loss: 1.7914 - val_acc: 0.1686\n",
      "\n",
      "Epoch 20/30\n",
      " - 6s - loss: 1.8051 - acc: 0.1877 - val_loss: 1.7917 - val_acc: 0.1805\n",
      "\n",
      "Epoch 21/30\n",
      " - 6s - loss: 1.8053 - acc: 0.1814 - val_loss: 1.7917 - val_acc: 0.1805\n",
      "\n",
      "Epoch 22/30\n",
      " - 6s - loss: 1.8051 - acc: 0.1903 - val_loss: 1.7918 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 6s - loss: 1.8068 - acc: 0.1863 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 6s - loss: 1.8076 - acc: 0.1840 - val_loss: 1.7918 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 6s - loss: 1.8073 - acc: 0.1816 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 6s - loss: 1.8045 - acc: 0.1828 - val_loss: 1.7918 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 6s - loss: 1.8102 - acc: 0.1862 - val_loss: 1.7918 - val_acc: 0.1805\n",
      "\n",
      "Epoch 28/30\n",
      " - 6s - loss: 1.8101 - acc: 0.1772 - val_loss: 1.7918 - val_acc: 0.0957\n",
      "\n",
      "Epoch 29/30\n",
      " - 6s - loss: 1.8101 - acc: 0.1805 - val_loss: 1.7918 - val_acc: 0.1249\n",
      "\n",
      "Epoch 30/30\n",
      " - 6s - loss: 1.8088 - acc: 0.1772 - val_loss: 1.7918 - val_acc: 0.1900\n",
      "\n",
      "Test accuracy:\n",
      "0.18798778418730913\n",
      " 10%|█         | 3/30 [07:27<58:29, 130.00s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 3s - loss: 1.7914 - acc: 0.1863 - val_loss: 1.7898 - val_acc: 0.1666\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.7902 - acc: 0.1859 - val_loss: 1.7937 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7874 - acc: 0.1848 - val_loss: 1.7923 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.7886 - acc: 0.1844 - val_loss: 1.8013 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.7877 - acc: 0.1897 - val_loss: 1.7897 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.7873 - acc: 0.1873 - val_loss: 1.7919 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.7870 - acc: 0.1922 - val_loss: 1.7896 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7872 - acc: 0.1903 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.7884 - acc: 0.1929 - val_loss: 1.7903 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.7869 - acc: 0.1900 - val_loss: 1.7923 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.7867 - acc: 0.1896 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.7873 - acc: 0.1903 - val_loss: 1.7903 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.7866 - acc: 0.1979 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.7880 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7886 - acc: 0.1906 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.7883 - acc: 0.1914 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.7888 - acc: 0.1929 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.7871 - acc: 0.1903 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7892 - acc: 0.1908 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.7929 - acc: 0.1915 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.7945 - acc: 0.1912 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.7946 - acc: 0.1910 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7914 - acc: 0.1911 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7904 - acc: 0.1912 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7912 - acc: 0.1915 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7898 - acc: 0.1915 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7888 - acc: 0.1916 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7909 - acc: 0.1916 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7889 - acc: 0.1925 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7909 - acc: 0.1918 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 13%|█▎        | 4/30 [07:47<42:02, 97.03s/it, best loss: -0.18798778418730913] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 4s - loss: 2.0523 - acc: 0.1706 - val_loss: 1.7890 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.9965 - acc: 0.1684 - val_loss: 1.7907 - val_acc: 0.1805\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.9710 - acc: 0.1785 - val_loss: 1.7918 - val_acc: 0.1805\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.9694 - acc: 0.1737 - val_loss: 1.7908 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.9600 - acc: 0.1727 - val_loss: 1.7924 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.9391 - acc: 0.1767 - val_loss: 1.7902 - val_acc: 0.1666\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.9470 - acc: 0.1723 - val_loss: 1.7907 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.9336 - acc: 0.1729 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.9313 - acc: 0.1666 - val_loss: 1.7905 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.9149 - acc: 0.1685 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.9170 - acc: 0.1654 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.9027 - acc: 0.1729 - val_loss: 1.7922 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.8984 - acc: 0.1748 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.8884 - acc: 0.1760 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.8951 - acc: 0.1673 - val_loss: 1.7904 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.8820 - acc: 0.1719 - val_loss: 1.7912 - val_acc: 0.1805\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.8861 - acc: 0.1745 - val_loss: 1.7905 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.8725 - acc: 0.1794 - val_loss: 1.7911 - val_acc: 0.1666\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.8649 - acc: 0.1723 - val_loss: 1.7904 - val_acc: 0.1805\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.8710 - acc: 0.1708 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.8668 - acc: 0.1657 - val_loss: 1.7897 - val_acc: 0.1805\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.8571 - acc: 0.1742 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.8542 - acc: 0.1793 - val_loss: 1.7918 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.8545 - acc: 0.1703 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.8494 - acc: 0.1744 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.8449 - acc: 0.1708 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.8393 - acc: 0.1734 - val_loss: 1.7902 - val_acc: 0.1805\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.8371 - acc: 0.1801 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.8405 - acc: 0.1726 - val_loss: 1.7912 - val_acc: 0.1805\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.8328 - acc: 0.1791 - val_loss: 1.7912 - val_acc: 0.1805\n",
      "\n",
      "Test accuracy:\n",
      "0.18052256532066507\n",
      " 17%|█▋        | 5/30 [08:25<33:05, 79.40s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 4s - loss: 1.7911 - acc: 0.1808 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.7896 - acc: 0.1881 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7885 - acc: 0.1887 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.7875 - acc: 0.1896 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.7867 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.7860 - acc: 0.1910 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.7855 - acc: 0.1912 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7852 - acc: 0.1916 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.7850 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.7847 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.7845 - acc: 0.1911 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.7844 - acc: 0.1914 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.7843 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.7842 - acc: 0.1914 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7841 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.7841 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 20%|██        | 6/30 [09:12<27:50, 69.61s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.7915 - acc: 0.1760 - val_loss: 1.7913 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 4s - loss: 1.7906 - acc: 0.1870 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 4s - loss: 1.7899 - acc: 0.1931 - val_loss: 1.7905 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 4s - loss: 1.7894 - acc: 0.1885 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 4s - loss: 1.7890 - acc: 0.1843 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 4s - loss: 1.7882 - acc: 0.1945 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 4s - loss: 1.7878 - acc: 0.1895 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 4s - loss: 1.7877 - acc: 0.1897 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 4s - loss: 1.7869 - acc: 0.1933 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 4s - loss: 1.7871 - acc: 0.1866 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 4s - loss: 1.7862 - acc: 0.1885 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 4s - loss: 1.7865 - acc: 0.1857 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 4s - loss: 1.7864 - acc: 0.1887 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 4s - loss: 1.7859 - acc: 0.1896 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 4s - loss: 1.7859 - acc: 0.1921 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 4s - loss: 1.7857 - acc: 0.1902 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 4s - loss: 1.7856 - acc: 0.1882 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 4s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 4s - loss: 1.7856 - acc: 0.1923 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 4s - loss: 1.7853 - acc: 0.1893 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 4s - loss: 1.7861 - acc: 0.1915 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 4s - loss: 1.7849 - acc: 0.1915 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 4s - loss: 1.7856 - acc: 0.1916 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 4s - loss: 1.7852 - acc: 0.1916 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 4s - loss: 1.7849 - acc: 0.1906 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 4s - loss: 1.7863 - acc: 0.1918 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 4s - loss: 1.7863 - acc: 0.1918 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 4s - loss: 1.7860 - acc: 0.1938 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 4s - loss: 1.7854 - acc: 0.1940 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 4s - loss: 1.7856 - acc: 0.1892 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 23%|██▎       | 7/30 [11:15<32:48, 85.57s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 1.8338 - acc: 0.1790 - val_loss: 1.8388 - val_acc: 0.1666\n",
      "\n",
      "Epoch 2/30\n",
      " - 2s - loss: 1.8184 - acc: 0.1813 - val_loss: 1.8125 - val_acc: 0.1666\n",
      "\n",
      "Epoch 3/30\n",
      " - 2s - loss: 1.8098 - acc: 0.1776 - val_loss: 1.8113 - val_acc: 0.1666\n",
      "\n",
      "Epoch 4/30\n",
      " - 2s - loss: 1.8005 - acc: 0.1832 - val_loss: 1.8224 - val_acc: 0.1598\n",
      "\n",
      "Epoch 5/30\n",
      " - 2s - loss: 1.7990 - acc: 0.1723 - val_loss: 1.8079 - val_acc: 0.1666\n",
      "\n",
      "Epoch 6/30\n",
      " - 2s - loss: 1.7953 - acc: 0.1810 - val_loss: 1.8103 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 2s - loss: 1.7941 - acc: 0.1840 - val_loss: 1.7931 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1812 - val_loss: 1.7922 - val_acc: 0.1666\n",
      "\n",
      "Epoch 9/30\n",
      " - 2s - loss: 1.7895 - acc: 0.1842 - val_loss: 1.7931 - val_acc: 0.1805\n",
      "\n",
      "Epoch 10/30\n",
      " - 2s - loss: 1.7895 - acc: 0.1775 - val_loss: 1.7913 - val_acc: 0.1805\n",
      "\n",
      "Epoch 11/30\n",
      " - 2s - loss: 1.7895 - acc: 0.1836 - val_loss: 1.7928 - val_acc: 0.1666\n",
      "\n",
      "Epoch 12/30\n",
      " - 2s - loss: 1.7878 - acc: 0.1884 - val_loss: 1.7897 - val_acc: 0.1683\n",
      "\n",
      "Epoch 13/30\n",
      " - 2s - loss: 1.7878 - acc: 0.1772 - val_loss: 1.7898 - val_acc: 0.1666\n",
      "\n",
      "Epoch 14/30\n",
      " - 2s - loss: 1.7873 - acc: 0.1854 - val_loss: 1.7939 - val_acc: 0.1683\n",
      "\n",
      "Epoch 15/30\n",
      " - 2s - loss: 1.7867 - acc: 0.1862 - val_loss: 1.7921 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 2s - loss: 1.7865 - acc: 0.1848 - val_loss: 1.7907 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 2s - loss: 1.7868 - acc: 0.1880 - val_loss: 1.7938 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 2s - loss: 1.7863 - acc: 0.1853 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 2s - loss: 1.7865 - acc: 0.1881 - val_loss: 1.7927 - val_acc: 0.1805\n",
      "\n",
      "Epoch 20/30\n",
      " - 2s - loss: 1.7865 - acc: 0.1817 - val_loss: 1.7903 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 2s - loss: 1.7854 - acc: 0.1931 - val_loss: 1.7915 - val_acc: 0.1805\n",
      "\n",
      "Epoch 22/30\n",
      " - 2s - loss: 1.7858 - acc: 0.1884 - val_loss: 1.7921 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 2s - loss: 1.7858 - acc: 0.1888 - val_loss: 1.7915 - val_acc: 0.1805\n",
      "\n",
      "Epoch 24/30\n",
      " - 2s - loss: 1.7853 - acc: 0.1832 - val_loss: 1.7926 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 2s - loss: 1.7857 - acc: 0.1850 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 2s - loss: 1.7846 - acc: 0.1842 - val_loss: 1.7894 - val_acc: 0.1666\n",
      "\n",
      "Epoch 27/30\n",
      " - 2s - loss: 1.7851 - acc: 0.1925 - val_loss: 1.7934 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 2s - loss: 1.7853 - acc: 0.1866 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 2s - loss: 1.7855 - acc: 0.1842 - val_loss: 1.7897 - val_acc: 0.1805\n",
      "\n",
      "Epoch 30/30\n",
      " - 2s - loss: 1.7852 - acc: 0.1802 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 27%|██▋       | 8/30 [12:27<29:54, 81.56s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 4s - loss: 1.7917 - acc: 0.1751 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.7917 - acc: 0.1779 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7917 - acc: 0.1919 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.7916 - acc: 0.1910 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.7916 - acc: 0.1919 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.7915 - acc: 0.1912 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.7915 - acc: 0.1927 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7915 - acc: 0.1903 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.7914 - acc: 0.1914 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.7914 - acc: 0.1922 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.7914 - acc: 0.1915 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.7913 - acc: 0.1910 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.7913 - acc: 0.1912 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.7913 - acc: 0.1912 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7912 - acc: 0.1915 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.7912 - acc: 0.1915 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.7911 - acc: 0.1916 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.7911 - acc: 0.1911 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7911 - acc: 0.1912 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.7910 - acc: 0.1911 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.7910 - acc: 0.1915 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.7910 - acc: 0.1914 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7909 - acc: 0.1914 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7909 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7909 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7908 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7908 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7908 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7907 - acc: 0.1914 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7907 - acc: 0.1914 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 30%|███       | 9/30 [12:49<22:18, 63.72s/it, best loss: -0.18798778418730913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 1.7898 - acc: 0.1805 - val_loss: 1.7894 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.7879 - acc: 0.1785 - val_loss: 1.7889 - val_acc: 0.1805\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7870 - acc: 0.1888 - val_loss: 1.7887 - val_acc: 0.1805\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.7860 - acc: 0.1843 - val_loss: 1.7888 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.7860 - acc: 0.1854 - val_loss: 1.7888 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.7858 - acc: 0.1972 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.7858 - acc: 0.1922 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7861 - acc: 0.1873 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.7851 - acc: 0.1941 - val_loss: 1.7886 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.7847 - acc: 0.1902 - val_loss: 1.7884 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.7847 - acc: 0.1984 - val_loss: 1.7884 - val_acc: 0.2019\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.7834 - acc: 0.2046 - val_loss: 1.7884 - val_acc: 0.1846\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.7830 - acc: 0.2024 - val_loss: 1.7881 - val_acc: 0.1805\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.7821 - acc: 0.2029 - val_loss: 1.7883 - val_acc: 0.2175\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7802 - acc: 0.2153 - val_loss: 1.7890 - val_acc: 0.1781\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.7791 - acc: 0.2101 - val_loss: 1.7876 - val_acc: 0.1992\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.7770 - acc: 0.2074 - val_loss: 1.7872 - val_acc: 0.2002\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.7747 - acc: 0.2084 - val_loss: 1.7862 - val_acc: 0.2063\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7741 - acc: 0.2095 - val_loss: 1.7863 - val_acc: 0.1961\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.7731 - acc: 0.2077 - val_loss: 1.7846 - val_acc: 0.1992\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.7669 - acc: 0.2201 - val_loss: 1.7824 - val_acc: 0.2043\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.7677 - acc: 0.2135 - val_loss: 1.7801 - val_acc: 0.2121\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7633 - acc: 0.2258 - val_loss: 1.7767 - val_acc: 0.2202\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7637 - acc: 0.2210 - val_loss: 1.7818 - val_acc: 0.1907\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7580 - acc: 0.2286 - val_loss: 1.7782 - val_acc: 0.1968\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7557 - acc: 0.2308 - val_loss: 1.7826 - val_acc: 0.1870\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7541 - acc: 0.2416 - val_loss: 1.7821 - val_acc: 0.1870\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7527 - acc: 0.2395 - val_loss: 1.7788 - val_acc: 0.1900\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7482 - acc: 0.2395 - val_loss: 1.7787 - val_acc: 0.1900\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7436 - acc: 0.2432 - val_loss: 1.7744 - val_acc: 0.1907\n",
      "\n",
      "Test accuracy:\n",
      "0.19070240922972514\n",
      " 33%|███▎      | 10/30 [13:36<19:32, 58.62s/it, best loss: -0.19070240922972514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 1.8133 - acc: 0.1636 - val_loss: 1.7918 - val_acc: 0.1591\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.8009 - acc: 0.1684 - val_loss: 1.7918 - val_acc: 0.1598\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.8038 - acc: 0.1639 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.8077 - acc: 0.1651 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.8036 - acc: 0.1619 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.8122 - acc: 0.1620 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.8088 - acc: 0.1623 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7989 - acc: 0.1644 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.8054 - acc: 0.1640 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.8051 - acc: 0.1623 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.8080 - acc: 0.1695 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.8022 - acc: 0.1624 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.8039 - acc: 0.1676 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.8009 - acc: 0.1636 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7966 - acc: 0.1647 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.8047 - acc: 0.1640 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.8015 - acc: 0.1644 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.8074 - acc: 0.1643 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7958 - acc: 0.1627 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.8010 - acc: 0.1619 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.8026 - acc: 0.1683 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.8091 - acc: 0.1669 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7971 - acc: 0.1651 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7998 - acc: 0.1620 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.8123 - acc: 0.1602 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.8027 - acc: 0.1651 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7971 - acc: 0.1639 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.8025 - acc: 0.1657 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7958 - acc: 0.1559 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7953 - acc: 0.1594 - val_loss: 1.7918 - val_acc: 0.1683\n",
      "\n",
      "Test accuracy:\n",
      "0.168306752629793\n",
      " 37%|███▋      | 11/30 [14:25<17:43, 56.00s/it, best loss: -0.19070240922972514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 7s - loss: 1.7909 - acc: 0.1850 - val_loss: 1.7908 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 3s - loss: 1.7895 - acc: 0.1921 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 3s - loss: 1.7882 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 3s - loss: 1.7874 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 3s - loss: 1.7866 - acc: 0.1912 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 3s - loss: 1.7860 - acc: 0.1914 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 3s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 3s - loss: 1.7852 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 3s - loss: 1.7849 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 3s - loss: 1.7847 - acc: 0.1914 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 3s - loss: 1.7845 - acc: 0.1914 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 3s - loss: 1.7844 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 3s - loss: 1.7843 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 3s - loss: 1.7842 - acc: 0.1914 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 3s - loss: 1.7842 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 3s - loss: 1.7839 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 3s - loss: 1.7840 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 40%|████      | 12/30 [15:51<19:28, 64.91s/it, best loss: -0.19070240922972514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 3.7009 - acc: 0.1689 - val_loss: 1.7966 - val_acc: 0.1666\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 3.4078 - acc: 0.1717 - val_loss: 1.7970 - val_acc: 0.1805\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 3.1306 - acc: 0.1779 - val_loss: 1.7981 - val_acc: 0.1805\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 2.9126 - acc: 0.1755 - val_loss: 1.7976 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 2.7381 - acc: 0.1733 - val_loss: 1.7974 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 2.5777 - acc: 0.1757 - val_loss: 1.8031 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 2.4324 - acc: 0.1639 - val_loss: 1.7991 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 2.2824 - acc: 0.1688 - val_loss: 1.8038 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 2.1970 - acc: 0.1653 - val_loss: 1.7974 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 2.0978 - acc: 0.1695 - val_loss: 1.7976 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 2.0098 - acc: 0.1674 - val_loss: 1.7950 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.9805 - acc: 0.1663 - val_loss: 1.7945 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.9177 - acc: 0.1755 - val_loss: 1.7921 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.8881 - acc: 0.1693 - val_loss: 1.7931 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.8513 - acc: 0.1779 - val_loss: 1.7910 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.8491 - acc: 0.1719 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.8298 - acc: 0.1689 - val_loss: 1.7910 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.8265 - acc: 0.1672 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.8180 - acc: 0.1680 - val_loss: 1.7922 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.8079 - acc: 0.1718 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.8007 - acc: 0.1848 - val_loss: 1.7906 - val_acc: 0.1805\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.8001 - acc: 0.1797 - val_loss: 1.7905 - val_acc: 0.1805\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7977 - acc: 0.1844 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7980 - acc: 0.1775 - val_loss: 1.7908 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7947 - acc: 0.1836 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7930 - acc: 0.1895 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7948 - acc: 0.1846 - val_loss: 1.7902 - val_acc: 0.1805\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7943 - acc: 0.1824 - val_loss: 1.7905 - val_acc: 0.1805\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7907 - acc: 0.1874 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7942 - acc: 0.1774 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 43%|████▎     | 13/30 [16:16<14:56, 52.76s/it, best loss: -0.19070240922972514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 7s - loss: 1.8126 - acc: 0.1761 - val_loss: 1.7888 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.7890 - acc: 0.1794 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7890 - acc: 0.1882 - val_loss: 1.7890 - val_acc: 0.1805\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.7880 - acc: 0.1797 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.7884 - acc: 0.1866 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.7893 - acc: 0.1846 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.7881 - acc: 0.1819 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.7879 - acc: 0.1857 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.7875 - acc: 0.1857 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.7875 - acc: 0.1896 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.7883 - acc: 0.1800 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.7884 - acc: 0.1936 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.7874 - acc: 0.1902 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.7879 - acc: 0.1854 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.7871 - acc: 0.1972 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.7880 - acc: 0.1903 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.7869 - acc: 0.1957 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.7873 - acc: 0.1895 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.7870 - acc: 0.1900 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.7881 - acc: 0.1885 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.7877 - acc: 0.1897 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.7880 - acc: 0.1891 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.7870 - acc: 0.1914 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.7867 - acc: 0.1944 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.7879 - acc: 0.1870 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.7877 - acc: 0.1908 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.7864 - acc: 0.1835 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.7868 - acc: 0.1925 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.7869 - acc: 0.1911 - val_loss: 1.7889 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.7877 - acc: 0.1927 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 47%|████▋     | 14/30 [16:59<13:17, 49.86s/it, best loss: -0.19070240922972514]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.9715 - acc: 0.1820 - val_loss: 1.6817 - val_acc: 0.3471\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.8266 - acc: 0.2325 - val_loss: 1.5779 - val_acc: 0.3278\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.7400 - acc: 0.2333 - val_loss: 1.5278 - val_acc: 0.2786\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.6843 - acc: 0.2436 - val_loss: 1.4079 - val_acc: 0.3475\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.6136 - acc: 0.2655 - val_loss: 1.3866 - val_acc: 0.3488\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.5387 - acc: 0.2822 - val_loss: 1.4556 - val_acc: 0.3627\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.4763 - acc: 0.2867 - val_loss: 1.3219 - val_acc: 0.3342\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.3980 - acc: 0.3183 - val_loss: 1.4129 - val_acc: 0.3627\n",
      "\n",
      "Epoch 9/30\n",
      " - 1s - loss: 1.3405 - acc: 0.3273 - val_loss: 1.1946 - val_acc: 0.3563\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.2920 - acc: 0.3353 - val_loss: 1.2312 - val_acc: 0.3386\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.2638 - acc: 0.3507 - val_loss: 1.2522 - val_acc: 0.3482\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.2474 - acc: 0.3339 - val_loss: 1.4044 - val_acc: 0.2901\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.2325 - acc: 0.3429 - val_loss: 1.3516 - val_acc: 0.3156\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.2226 - acc: 0.3400 - val_loss: 1.2578 - val_acc: 0.3258\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.2076 - acc: 0.3478 - val_loss: 1.3045 - val_acc: 0.3091\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.2004 - acc: 0.3468 - val_loss: 1.3726 - val_acc: 0.3627\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.1936 - acc: 0.3483 - val_loss: 1.2479 - val_acc: 0.3359\n",
      "\n",
      "Epoch 18/30\n",
      " - 1s - loss: 1.1916 - acc: 0.3464 - val_loss: 1.3082 - val_acc: 0.3339\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.1901 - acc: 0.3611 - val_loss: 1.2500 - val_acc: 0.3566\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.1851 - acc: 0.3569 - val_loss: 1.4420 - val_acc: 0.3627\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.1847 - acc: 0.3555 - val_loss: 1.2001 - val_acc: 0.3577\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.1833 - acc: 0.3434 - val_loss: 1.2830 - val_acc: 0.3431\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.1809 - acc: 0.3538 - val_loss: 1.2141 - val_acc: 0.3437\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.1832 - acc: 0.3477 - val_loss: 1.4750 - val_acc: 0.3078\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.1769 - acc: 0.3536 - val_loss: 1.2212 - val_acc: 0.3359\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.1806 - acc: 0.3462 - val_loss: 1.2929 - val_acc: 0.3366\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.1872 - acc: 0.3512 - val_loss: 1.1748 - val_acc: 0.3325\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.1738 - acc: 0.3486 - val_loss: 1.2087 - val_acc: 0.3376\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.1699 - acc: 0.3576 - val_loss: 1.1945 - val_acc: 0.3627\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.1732 - acc: 0.3615 - val_loss: 1.3678 - val_acc: 0.3363\n",
      "\n",
      "Test accuracy:\n",
      "0.33627417712928404\n",
      " 50%|█████     | 15/30 [17:45<12:11, 48.78s/it, best loss: -0.33627417712928404]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 6s - loss: 1.9157 - acc: 0.1643 - val_loss: 1.7943 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 1s - loss: 1.8433 - acc: 0.1719 - val_loss: 1.7908 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 1s - loss: 1.8356 - acc: 0.1801 - val_loss: 1.7905 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 1s - loss: 1.8356 - acc: 0.1794 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 1s - loss: 1.8428 - acc: 0.1726 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 1s - loss: 1.8430 - acc: 0.1683 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 1s - loss: 1.8376 - acc: 0.1737 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 1s - loss: 1.8356 - acc: 0.1774 - val_loss: 1.7902 - val_acc: 0.1805\n",
      "\n",
      "Epoch 9/30\n",
      " - 2s - loss: 1.8386 - acc: 0.1730 - val_loss: 1.7909 - val_acc: 0.1805\n",
      "\n",
      "Epoch 10/30\n",
      " - 1s - loss: 1.8327 - acc: 0.1794 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 1s - loss: 1.8374 - acc: 0.1748 - val_loss: 1.7889 - val_acc: 0.1805\n",
      "\n",
      "Epoch 12/30\n",
      " - 1s - loss: 1.8423 - acc: 0.1697 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 1s - loss: 1.8337 - acc: 0.1744 - val_loss: 1.7907 - val_acc: 0.1805\n",
      "\n",
      "Epoch 14/30\n",
      " - 1s - loss: 1.8347 - acc: 0.1768 - val_loss: 1.7911 - val_acc: 0.1805\n",
      "\n",
      "Epoch 15/30\n",
      " - 1s - loss: 1.8294 - acc: 0.1819 - val_loss: 1.7918 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 1s - loss: 1.8331 - acc: 0.1761 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 1s - loss: 1.8320 - acc: 0.1800 - val_loss: 1.7899 - val_acc: 0.1805\n",
      "\n",
      "Epoch 18/30\n",
      " - 2s - loss: 1.8311 - acc: 0.1766 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 1s - loss: 1.8350 - acc: 0.1703 - val_loss: 1.7921 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 1s - loss: 1.8335 - acc: 0.1791 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 1s - loss: 1.8299 - acc: 0.1771 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 1s - loss: 1.8263 - acc: 0.1825 - val_loss: 1.7907 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 1s - loss: 1.8288 - acc: 0.1770 - val_loss: 1.7909 - val_acc: 0.1805\n",
      "\n",
      "Epoch 24/30\n",
      " - 1s - loss: 1.8326 - acc: 0.1797 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 1s - loss: 1.8301 - acc: 0.1816 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 1s - loss: 1.8330 - acc: 0.1711 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 1s - loss: 1.8250 - acc: 0.1817 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 1s - loss: 1.8293 - acc: 0.1696 - val_loss: 1.7899 - val_acc: 0.1805\n",
      "\n",
      "Epoch 29/30\n",
      " - 1s - loss: 1.8287 - acc: 0.1746 - val_loss: 1.7924 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 1s - loss: 1.8290 - acc: 0.1703 - val_loss: 1.7907 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 53%|█████▎    | 16/30 [18:37<11:35, 49.69s/it, best loss: -0.33627417712928404]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 9s - loss: 1.7918 - acc: 0.1647 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1828 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1859 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1846 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1846 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 2s - loss: 1.7917 - acc: 0.1933 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1891 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1912 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1926 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1906 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1900 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 2s - loss: 1.7916 - acc: 0.1900 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 2s - loss: 1.7915 - acc: 0.1889 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 2s - loss: 1.7915 - acc: 0.1908 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 2s - loss: 1.7915 - acc: 0.1902 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 2s - loss: 1.7915 - acc: 0.1908 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 2s - loss: 1.7915 - acc: 0.1908 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1919 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1916 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1921 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1914 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1910 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 2s - loss: 1.7914 - acc: 0.1907 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1915 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1911 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1914 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1912 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1915 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 2s - loss: 1.7913 - acc: 0.1915 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 2s - loss: 1.7912 - acc: 0.1915 - val_loss: 1.7914 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 57%|█████▋    | 17/30 [19:46<12:02, 55.60s/it, best loss: -0.33627417712928404]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 13s - loss: 1.8045 - acc: 0.1906 - val_loss: 1.7663 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 8s - loss: 1.7881 - acc: 0.1952 - val_loss: 1.7432 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 8s - loss: 1.7575 - acc: 0.2267 - val_loss: 1.7142 - val_acc: 0.3505\n",
      "\n",
      "Epoch 4/30\n",
      " - 8s - loss: 1.7265 - acc: 0.2384 - val_loss: 1.6694 - val_acc: 0.3475\n",
      "\n",
      "Epoch 5/30\n",
      " - 8s - loss: 1.6982 - acc: 0.2489 - val_loss: 1.6111 - val_acc: 0.3505\n",
      "\n",
      "Epoch 6/30\n",
      " - 8s - loss: 1.6553 - acc: 0.2625 - val_loss: 1.5460 - val_acc: 0.3509\n",
      "\n",
      "Epoch 7/30\n",
      " - 8s - loss: 1.6041 - acc: 0.2773 - val_loss: 1.4781 - val_acc: 0.3505\n",
      "\n",
      "Epoch 8/30\n",
      " - 8s - loss: 1.5610 - acc: 0.2813 - val_loss: 1.4141 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 8s - loss: 1.5319 - acc: 0.2779 - val_loss: 1.3615 - val_acc: 0.3505\n",
      "\n",
      "Epoch 10/30\n",
      " - 8s - loss: 1.4975 - acc: 0.2905 - val_loss: 1.3170 - val_acc: 0.3505\n",
      "\n",
      "Epoch 11/30\n",
      " - 8s - loss: 1.4680 - acc: 0.2889 - val_loss: 1.2937 - val_acc: 0.3526\n",
      "\n",
      "Epoch 12/30\n",
      " - 8s - loss: 1.4424 - acc: 0.3006 - val_loss: 1.2622 - val_acc: 0.3461\n",
      "\n",
      "Epoch 13/30\n",
      " - 8s - loss: 1.4272 - acc: 0.2942 - val_loss: 1.2328 - val_acc: 0.3505\n",
      "\n",
      "Epoch 14/30\n",
      " - 8s - loss: 1.4106 - acc: 0.2866 - val_loss: 1.2118 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 8s - loss: 1.4143 - acc: 0.2938 - val_loss: 1.1991 - val_acc: 0.3485\n",
      "\n",
      "Epoch 16/30\n",
      " - 8s - loss: 1.3911 - acc: 0.2949 - val_loss: 1.1898 - val_acc: 0.3468\n",
      "\n",
      "Epoch 17/30\n",
      " - 8s - loss: 1.3881 - acc: 0.2956 - val_loss: 1.1760 - val_acc: 0.3505\n",
      "\n",
      "Epoch 18/30\n",
      " - 8s - loss: 1.3780 - acc: 0.2949 - val_loss: 1.1671 - val_acc: 0.3482\n",
      "\n",
      "Epoch 19/30\n",
      " - 8s - loss: 1.3683 - acc: 0.2937 - val_loss: 1.1673 - val_acc: 0.3505\n",
      "\n",
      "Epoch 20/30\n",
      " - 8s - loss: 1.3655 - acc: 0.2999 - val_loss: 1.1519 - val_acc: 0.3505\n",
      "\n",
      "Epoch 21/30\n",
      " - 8s - loss: 1.3494 - acc: 0.2931 - val_loss: 1.1492 - val_acc: 0.3485\n",
      "\n",
      "Epoch 22/30\n",
      " - 8s - loss: 1.3626 - acc: 0.2935 - val_loss: 1.1450 - val_acc: 0.3505\n",
      "\n",
      "Epoch 23/30\n",
      " - 8s - loss: 1.3500 - acc: 0.2899 - val_loss: 1.1397 - val_acc: 0.3505\n",
      "\n",
      "Epoch 24/30\n",
      " - 8s - loss: 1.3573 - acc: 0.2920 - val_loss: 1.1359 - val_acc: 0.3505\n",
      "\n",
      "Epoch 25/30\n",
      " - 8s - loss: 1.3542 - acc: 0.2964 - val_loss: 1.1348 - val_acc: 0.3488\n",
      "\n",
      "Epoch 26/30\n",
      " - 8s - loss: 1.3451 - acc: 0.2928 - val_loss: 1.1499 - val_acc: 0.3505\n",
      "\n",
      "Epoch 27/30\n",
      " - 8s - loss: 1.3377 - acc: 0.2947 - val_loss: 1.1299 - val_acc: 0.3488\n",
      "\n",
      "Epoch 28/30\n",
      " - 8s - loss: 1.3517 - acc: 0.2969 - val_loss: 1.1266 - val_acc: 0.3488\n",
      "\n",
      "Epoch 29/30\n",
      " - 8s - loss: 1.3383 - acc: 0.2911 - val_loss: 1.1251 - val_acc: 0.3505\n",
      "\n",
      "Epoch 30/30\n",
      " - 8s - loss: 1.3433 - acc: 0.2882 - val_loss: 1.1252 - val_acc: 0.3505\n",
      "\n",
      "Test accuracy:\n",
      "0.3505259586019681\n",
      " 60%|██████    | 18/30 [23:54<22:40, 113.35s/it, best loss: -0.3505259586019681]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 10s - loss: 1.7867 - acc: 0.1900 - val_loss: 1.7830 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 4s - loss: 1.7666 - acc: 0.2194 - val_loss: 1.7482 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 4s - loss: 1.7221 - acc: 0.2829 - val_loss: 1.6757 - val_acc: 0.3505\n",
      "\n",
      "Epoch 4/30\n",
      " - 4s - loss: 1.6692 - acc: 0.2794 - val_loss: 1.5865 - val_acc: 0.3505\n",
      "\n",
      "Epoch 5/30\n",
      " - 5s - loss: 1.6158 - acc: 0.2817 - val_loss: 1.4960 - val_acc: 0.3505\n",
      "\n",
      "Epoch 6/30\n",
      " - 4s - loss: 1.5725 - acc: 0.2841 - val_loss: 1.4160 - val_acc: 0.3502\n",
      "\n",
      "Epoch 7/30\n",
      " - 4s - loss: 1.5422 - acc: 0.2802 - val_loss: 1.3491 - val_acc: 0.3505\n",
      "\n",
      "Epoch 8/30\n",
      " - 4s - loss: 1.5135 - acc: 0.2831 - val_loss: 1.2978 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 4s - loss: 1.4982 - acc: 0.2810 - val_loss: 1.2665 - val_acc: 0.3505\n",
      "\n",
      "Epoch 10/30\n",
      " - 4s - loss: 1.4834 - acc: 0.2795 - val_loss: 1.2317 - val_acc: 0.3505\n",
      "\n",
      "Epoch 11/30\n",
      " - 4s - loss: 1.4666 - acc: 0.2803 - val_loss: 1.2063 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 4s - loss: 1.4554 - acc: 0.2816 - val_loss: 1.1897 - val_acc: 0.3505\n",
      "\n",
      "Epoch 13/30\n",
      " - 5s - loss: 1.4544 - acc: 0.2801 - val_loss: 1.1761 - val_acc: 0.3505\n",
      "\n",
      "Epoch 14/30\n",
      " - 4s - loss: 1.4485 - acc: 0.2801 - val_loss: 1.1642 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 4s - loss: 1.4394 - acc: 0.2848 - val_loss: 1.1599 - val_acc: 0.3505\n",
      "\n",
      "Epoch 16/30\n",
      " - 5s - loss: 1.4436 - acc: 0.2741 - val_loss: 1.1496 - val_acc: 0.3505\n",
      "\n",
      "Epoch 17/30\n",
      " - 4s - loss: 1.4356 - acc: 0.2797 - val_loss: 1.1458 - val_acc: 0.3505\n",
      "\n",
      "Epoch 18/30\n",
      " - 5s - loss: 1.4358 - acc: 0.2807 - val_loss: 1.1390 - val_acc: 0.3505\n",
      "\n",
      "Epoch 19/30\n",
      " - 4s - loss: 1.4303 - acc: 0.2790 - val_loss: 1.1338 - val_acc: 0.3505\n",
      "\n",
      "Epoch 20/30\n",
      " - 4s - loss: 1.4338 - acc: 0.2750 - val_loss: 1.1315 - val_acc: 0.3505\n",
      "\n",
      "Epoch 21/30\n",
      " - 4s - loss: 1.4263 - acc: 0.2810 - val_loss: 1.1275 - val_acc: 0.3505\n",
      "\n",
      "Epoch 22/30\n",
      " - 4s - loss: 1.4343 - acc: 0.2777 - val_loss: 1.1309 - val_acc: 0.3505\n",
      "\n",
      "Epoch 23/30\n",
      " - 4s - loss: 1.4284 - acc: 0.2818 - val_loss: 1.1216 - val_acc: 0.3505\n",
      "\n",
      "Epoch 24/30\n",
      " - 4s - loss: 1.4224 - acc: 0.2795 - val_loss: 1.1209 - val_acc: 0.3505\n",
      "\n",
      "Epoch 25/30\n",
      " - 4s - loss: 1.4249 - acc: 0.2754 - val_loss: 1.1192 - val_acc: 0.3505\n",
      "\n",
      "Epoch 26/30\n",
      " - 5s - loss: 1.4234 - acc: 0.2818 - val_loss: 1.1165 - val_acc: 0.3505\n",
      "\n",
      "Epoch 27/30\n",
      " - 5s - loss: 1.4183 - acc: 0.2837 - val_loss: 1.1192 - val_acc: 0.3505\n",
      "\n",
      "Epoch 28/30\n",
      " - 4s - loss: 1.4250 - acc: 0.2777 - val_loss: 1.1249 - val_acc: 0.3505\n",
      "\n",
      "Epoch 29/30\n",
      " - 4s - loss: 1.4343 - acc: 0.2786 - val_loss: 1.1747 - val_acc: 0.3410\n",
      "\n",
      "Epoch 30/30\n",
      " - 4s - loss: 1.4344 - acc: 0.2786 - val_loss: 1.2792 - val_acc: 0.3502\n",
      "\n",
      "Test accuracy:\n",
      "0.3501866304716661\n",
      " 63%|██████▎   | 19/30 [26:17<22:22, 122.07s/it, best loss: -0.3505259586019681]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 9s - loss: 1.7859 - acc: 0.1863 - val_loss: 1.7921 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 3s - loss: 1.7853 - acc: 0.1868 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 3s - loss: 1.7852 - acc: 0.1844 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 3s - loss: 1.7849 - acc: 0.1899 - val_loss: 1.7908 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 3s - loss: 1.7849 - acc: 0.1863 - val_loss: 1.7910 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1882 - val_loss: 1.7905 - val_acc: 0.1805\n",
      "\n",
      "Epoch 7/30\n",
      " - 3s - loss: 1.7847 - acc: 0.1888 - val_loss: 1.7925 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1918 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 3s - loss: 1.7851 - acc: 0.1891 - val_loss: 1.7900 - val_acc: 0.1805\n",
      "\n",
      "Epoch 10/30\n",
      " - 3s - loss: 1.7847 - acc: 0.1956 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1889 - val_loss: 1.7911 - val_acc: 0.1805\n",
      "\n",
      "Epoch 12/30\n",
      " - 3s - loss: 1.7852 - acc: 0.1911 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1861 - val_loss: 1.7913 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 3s - loss: 1.7851 - acc: 0.1814 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1916 - val_loss: 1.7900 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 3s - loss: 1.7851 - acc: 0.1885 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 3s - loss: 1.7850 - acc: 0.1912 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 3s - loss: 1.7849 - acc: 0.1896 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 3s - loss: 1.7853 - acc: 0.1888 - val_loss: 1.7913 - val_acc: 0.1805\n",
      "\n",
      "Epoch 20/30\n",
      " - 3s - loss: 1.7853 - acc: 0.1862 - val_loss: 1.7907 - val_acc: 0.1805\n",
      "\n",
      "Epoch 21/30\n",
      " - 3s - loss: 1.7852 - acc: 0.1887 - val_loss: 1.7910 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 3s - loss: 1.7854 - acc: 0.1859 - val_loss: 1.7894 - val_acc: 0.1805\n",
      "\n",
      "Epoch 23/30\n",
      " - 3s - loss: 1.7847 - acc: 0.1923 - val_loss: 1.7925 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 3s - loss: 1.7849 - acc: 0.1910 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 3s - loss: 1.7855 - acc: 0.1907 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 2s - loss: 1.7851 - acc: 0.1881 - val_loss: 1.7901 - val_acc: 0.1805\n",
      "\n",
      "Epoch 27/30\n",
      " - 2s - loss: 1.7851 - acc: 0.1809 - val_loss: 1.7918 - val_acc: 0.1805\n",
      "\n",
      "Epoch 28/30\n",
      " - 3s - loss: 1.7854 - acc: 0.1866 - val_loss: 1.7898 - val_acc: 0.1805\n",
      "\n",
      "Epoch 29/30\n",
      " - 3s - loss: 1.7852 - acc: 0.1847 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 3s - loss: 1.7845 - acc: 0.1965 - val_loss: 1.7921 - val_acc: 0.1805\n",
      "\n",
      "Test accuracy:\n",
      "0.18052256532066507\n",
      " 67%|██████▋   | 20/30 [27:42<18:29, 110.96s/it, best loss: -0.3505259586019681]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 11s - loss: 1.7984 - acc: 0.1733 - val_loss: 1.7920 - val_acc: 0.1425\n",
      "\n",
      "Epoch 2/30\n",
      " - 5s - loss: 1.7873 - acc: 0.1761 - val_loss: 1.7918 - val_acc: 0.1425\n",
      "\n",
      "Epoch 3/30\n",
      " - 5s - loss: 1.7873 - acc: 0.1757 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 5s - loss: 1.7871 - acc: 0.1768 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 5s - loss: 1.7870 - acc: 0.1749 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 5s - loss: 1.7876 - acc: 0.1706 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 5s - loss: 1.7875 - acc: 0.1733 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1764 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 5s - loss: 1.7882 - acc: 0.1697 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 5s - loss: 1.7883 - acc: 0.1708 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 5s - loss: 1.7870 - acc: 0.1753 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 5s - loss: 1.7892 - acc: 0.1794 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 5s - loss: 1.7877 - acc: 0.1760 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1797 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1778 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 5s - loss: 1.7879 - acc: 0.1756 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 5s - loss: 1.7880 - acc: 0.1763 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1816 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 5s - loss: 1.7874 - acc: 0.1726 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 5s - loss: 1.7880 - acc: 0.1751 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 5s - loss: 1.7879 - acc: 0.1737 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 5s - loss: 1.7874 - acc: 0.1821 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 5s - loss: 1.7876 - acc: 0.1843 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 5s - loss: 1.7879 - acc: 0.1844 - val_loss: 1.7915 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 5s - loss: 1.8014 - acc: 0.1802 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 5s - loss: 1.8159 - acc: 0.1729 - val_loss: 1.7917 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 5s - loss: 1.8287 - acc: 0.1785 - val_loss: 1.7916 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 5s - loss: 1.8216 - acc: 0.1726 - val_loss: 1.7916 - val_acc: 0.1425\n",
      "\n",
      "Epoch 29/30\n",
      " - 5s - loss: 1.8048 - acc: 0.1797 - val_loss: 1.7916 - val_acc: 0.1425\n",
      "\n",
      "Epoch 30/30\n",
      " - 5s - loss: 1.7881 - acc: 0.1844 - val_loss: 1.7916 - val_acc: 0.1425\n",
      "\n",
      "Test accuracy:\n",
      "0.14251781472684086\n",
      " 70%|███████   | 21/30 [30:18<18:40, 124.46s/it, best loss: -0.3505259586019681]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 15s - loss: 1.8180 - acc: 0.1787 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 9s - loss: 1.8048 - acc: 0.1835 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 9s - loss: 1.8010 - acc: 0.1764 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 9s - loss: 1.7958 - acc: 0.1805 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 9s - loss: 1.7945 - acc: 0.1844 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 9s - loss: 1.7917 - acc: 0.1865 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 9s - loss: 1.7911 - acc: 0.1906 - val_loss: 1.7892 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 9s - loss: 1.7898 - acc: 0.1870 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 9s - loss: 1.7894 - acc: 0.1881 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 9s - loss: 1.7903 - acc: 0.1821 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 9s - loss: 1.7895 - acc: 0.1873 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 9s - loss: 1.7864 - acc: 0.1857 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 9s - loss: 1.7872 - acc: 0.1907 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 9s - loss: 1.7889 - acc: 0.1906 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 9s - loss: 1.7885 - acc: 0.1912 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 9s - loss: 1.7875 - acc: 0.1902 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 9s - loss: 1.7878 - acc: 0.1899 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 9s - loss: 1.7870 - acc: 0.1825 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 9s - loss: 1.7875 - acc: 0.1885 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 9s - loss: 1.7880 - acc: 0.1831 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 9s - loss: 1.7860 - acc: 0.1872 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 9s - loss: 1.7874 - acc: 0.1817 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 9s - loss: 1.7875 - acc: 0.1866 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 9s - loss: 1.7876 - acc: 0.1884 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 9s - loss: 1.7877 - acc: 0.1809 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1991 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 9s - loss: 1.7873 - acc: 0.1848 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 9s - loss: 1.7871 - acc: 0.1900 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 9s - loss: 1.7849 - acc: 0.1834 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 9s - loss: 1.7868 - acc: 0.1870 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 73%|███████▎  | 22/30 [34:50<22:31, 168.88s/it, best loss: -0.3505259586019681]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 14s - loss: 1.7881 - acc: 0.1858 - val_loss: 1.7887 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 7s - loss: 1.7775 - acc: 0.1906 - val_loss: 1.7616 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 7s - loss: 1.7201 - acc: 0.2829 - val_loss: 1.6522 - val_acc: 0.3505\n",
      "\n",
      "Epoch 4/30\n",
      " - 7s - loss: 1.6289 - acc: 0.2949 - val_loss: 1.5077 - val_acc: 0.3505\n",
      "\n",
      "Epoch 5/30\n",
      " - 8s - loss: 1.5524 - acc: 0.2964 - val_loss: 1.3890 - val_acc: 0.3505\n",
      "\n",
      "Epoch 6/30\n",
      " - 7s - loss: 1.4881 - acc: 0.2961 - val_loss: 1.2962 - val_acc: 0.3505\n",
      "\n",
      "Epoch 7/30\n",
      " - 7s - loss: 1.4495 - acc: 0.2941 - val_loss: 1.2351 - val_acc: 0.3502\n",
      "\n",
      "Epoch 8/30\n",
      " - 7s - loss: 1.4188 - acc: 0.2931 - val_loss: 1.1949 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 7s - loss: 1.4055 - acc: 0.2954 - val_loss: 1.1637 - val_acc: 0.3505\n",
      "\n",
      "Epoch 10/30\n",
      " - 8s - loss: 1.3873 - acc: 0.2905 - val_loss: 1.1459 - val_acc: 0.3505\n",
      "\n",
      "Epoch 11/30\n",
      " - 7s - loss: 1.3811 - acc: 0.2939 - val_loss: 1.1302 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 7s - loss: 1.3651 - acc: 0.2968 - val_loss: 1.1207 - val_acc: 0.3505\n",
      "\n",
      "Epoch 13/30\n",
      " - 7s - loss: 1.3716 - acc: 0.2913 - val_loss: 1.1180 - val_acc: 0.3509\n",
      "\n",
      "Epoch 14/30\n",
      " - 7s - loss: 1.3693 - acc: 0.3002 - val_loss: 1.1102 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 7s - loss: 1.3682 - acc: 0.2829 - val_loss: 1.1864 - val_acc: 0.3505\n",
      "\n",
      "Epoch 16/30\n",
      " - 8s - loss: 1.3557 - acc: 0.3006 - val_loss: 1.1249 - val_acc: 0.3505\n",
      "\n",
      "Epoch 17/30\n",
      " - 7s - loss: 1.3110 - acc: 0.3330 - val_loss: 1.1266 - val_acc: 0.3505\n",
      "\n",
      "Epoch 18/30\n",
      " - 7s - loss: 1.2917 - acc: 0.3293 - val_loss: 1.1123 - val_acc: 0.3505\n",
      "\n",
      "Epoch 19/30\n",
      " - 7s - loss: 1.2518 - acc: 0.3279 - val_loss: 1.1391 - val_acc: 0.3505\n",
      "\n",
      "Epoch 20/30\n",
      " - 7s - loss: 1.2398 - acc: 0.3278 - val_loss: 1.1200 - val_acc: 0.3505\n",
      "\n",
      "Epoch 21/30\n",
      " - 7s - loss: 1.2184 - acc: 0.3294 - val_loss: 1.1118 - val_acc: 0.3505\n",
      "\n",
      "Epoch 22/30\n",
      " - 7s - loss: 1.2132 - acc: 0.3609 - val_loss: 1.1434 - val_acc: 0.3434\n",
      "\n",
      "Epoch 23/30\n",
      " - 7s - loss: 1.2107 - acc: 0.4168 - val_loss: 1.0886 - val_acc: 0.5310\n",
      "\n",
      "Epoch 24/30\n",
      " - 7s - loss: 1.1872 - acc: 0.4444 - val_loss: 1.0550 - val_acc: 0.5310\n",
      "\n",
      "Epoch 25/30\n",
      " - 7s - loss: 1.1569 - acc: 0.4441 - val_loss: 1.0012 - val_acc: 0.5310\n",
      "\n",
      "Epoch 26/30\n",
      " - 8s - loss: 1.1253 - acc: 0.4408 - val_loss: 0.9484 - val_acc: 0.5310\n",
      "\n",
      "Epoch 27/30\n",
      " - 7s - loss: 1.0918 - acc: 0.4457 - val_loss: 0.9065 - val_acc: 0.5310\n",
      "\n",
      "Epoch 28/30\n",
      " - 7s - loss: 1.0690 - acc: 0.4431 - val_loss: 0.8687 - val_acc: 0.5310\n",
      "\n",
      "Epoch 29/30\n",
      " - 7s - loss: 1.0461 - acc: 0.4384 - val_loss: 0.8542 - val_acc: 0.5307\n",
      "\n",
      "Epoch 30/30\n",
      " - 7s - loss: 1.0360 - acc: 0.4393 - val_loss: 0.8496 - val_acc: 0.5310\n",
      "\n",
      "Test accuracy:\n",
      "0.5310485239226331\n",
      " 77%|███████▋  | 23/30 [38:42<21:53, 187.71s/it, best loss: -0.5310485239226331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 16s - loss: 1.7965 - acc: 0.1436 - val_loss: 1.7917 - val_acc: 0.1683\n",
      "\n",
      "Epoch 2/30\n",
      " - 9s - loss: 1.7899 - acc: 0.1839 - val_loss: 1.7904 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 9s - loss: 1.7893 - acc: 0.1899 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 9s - loss: 1.7878 - acc: 0.1804 - val_loss: 1.7902 - val_acc: 0.1822\n",
      "\n",
      "Epoch 5/30\n",
      " - 9s - loss: 1.7856 - acc: 0.1820 - val_loss: 1.7919 - val_acc: 0.1822\n",
      "\n",
      "Epoch 6/30\n",
      " - 9s - loss: 1.7853 - acc: 0.1912 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 9s - loss: 1.7862 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 9s - loss: 1.7854 - acc: 0.1914 - val_loss: 1.7898 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 9s - loss: 1.7858 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 9s - loss: 1.7857 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 9s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7935 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 13/30\n",
      " - 8s - loss: 1.7860 - acc: 0.1914 - val_loss: 1.7897 - val_acc: 0.1822\n",
      "\n",
      "Epoch 14/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 15/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 16/30\n",
      " - 9s - loss: 1.7853 - acc: 0.1914 - val_loss: 1.7937 - val_acc: 0.1822\n",
      "\n",
      "Epoch 17/30\n",
      " - 9s - loss: 1.7857 - acc: 0.1914 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 18/30\n",
      " - 9s - loss: 1.7857 - acc: 0.1914 - val_loss: 1.7906 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 8s - loss: 1.7858 - acc: 0.1914 - val_loss: 1.7966 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 9s - loss: 1.7859 - acc: 0.1914 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 8s - loss: 1.7858 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 9s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 9s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7918 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 8s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7911 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 8s - loss: 1.7858 - acc: 0.1914 - val_loss: 1.7912 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 9s - loss: 1.7855 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 8s - loss: 1.7854 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 8s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7895 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 8s - loss: 1.7856 - acc: 0.1914 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      " 80%|████████  | 24/30 [43:08<21:07, 211.25s/it, best loss: -0.5310485239226331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 14s - loss: 1.7804 - acc: 0.1956 - val_loss: 1.7506 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 7s - loss: 1.7255 - acc: 0.2421 - val_loss: 1.6950 - val_acc: 0.3488\n",
      "\n",
      "Epoch 3/30\n",
      " - 7s - loss: 1.6613 - acc: 0.3082 - val_loss: 1.6204 - val_acc: 0.3505\n",
      "\n",
      "Epoch 4/30\n",
      " - 6s - loss: 1.5880 - acc: 0.3226 - val_loss: 1.5371 - val_acc: 0.3488\n",
      "\n",
      "Epoch 5/30\n",
      " - 7s - loss: 1.5125 - acc: 0.3214 - val_loss: 1.4598 - val_acc: 0.3346\n",
      "\n",
      "Epoch 6/30\n",
      " - 6s - loss: 1.4423 - acc: 0.3243 - val_loss: 1.3866 - val_acc: 0.3346\n",
      "\n",
      "Epoch 7/30\n",
      " - 6s - loss: 1.3894 - acc: 0.3190 - val_loss: 1.3281 - val_acc: 0.3475\n",
      "\n",
      "Epoch 8/30\n",
      " - 7s - loss: 1.3363 - acc: 0.3308 - val_loss: 1.2816 - val_acc: 0.3478\n",
      "\n",
      "Epoch 9/30\n",
      " - 7s - loss: 1.3094 - acc: 0.3285 - val_loss: 1.2436 - val_acc: 0.3485\n",
      "\n",
      "Epoch 10/30\n",
      " - 7s - loss: 1.2828 - acc: 0.3311 - val_loss: 1.2163 - val_acc: 0.3485\n",
      "\n",
      "Epoch 11/30\n",
      " - 6s - loss: 1.2634 - acc: 0.3267 - val_loss: 1.1966 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 6s - loss: 1.2537 - acc: 0.3241 - val_loss: 1.1798 - val_acc: 0.3505\n",
      "\n",
      "Epoch 13/30\n",
      " - 6s - loss: 1.2398 - acc: 0.3313 - val_loss: 1.1679 - val_acc: 0.3488\n",
      "\n",
      "Epoch 14/30\n",
      " - 7s - loss: 1.2301 - acc: 0.3308 - val_loss: 1.1573 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 7s - loss: 1.2198 - acc: 0.3236 - val_loss: 1.1503 - val_acc: 0.3505\n",
      "\n",
      "Epoch 16/30\n",
      " - 7s - loss: 1.2289 - acc: 0.3213 - val_loss: 1.1432 - val_acc: 0.3488\n",
      "\n",
      "Epoch 17/30\n",
      " - 7s - loss: 1.2281 - acc: 0.3184 - val_loss: 1.1396 - val_acc: 0.3488\n",
      "\n",
      "Epoch 18/30\n",
      " - 6s - loss: 1.2182 - acc: 0.3217 - val_loss: 1.1381 - val_acc: 0.3505\n",
      "\n",
      "Epoch 19/30\n",
      " - 6s - loss: 1.2159 - acc: 0.3222 - val_loss: 1.1322 - val_acc: 0.3488\n",
      "\n",
      "Epoch 20/30\n",
      " - 6s - loss: 1.2106 - acc: 0.3312 - val_loss: 1.1314 - val_acc: 0.3485\n",
      "\n",
      "Epoch 21/30\n",
      " - 6s - loss: 1.2130 - acc: 0.3285 - val_loss: 1.1270 - val_acc: 0.3342\n",
      "\n",
      "Epoch 22/30\n",
      " - 6s - loss: 1.2125 - acc: 0.3249 - val_loss: 1.1285 - val_acc: 0.3505\n",
      "\n",
      "Epoch 23/30\n",
      " - 6s - loss: 1.2060 - acc: 0.3307 - val_loss: 1.1243 - val_acc: 0.3505\n",
      "\n",
      "Epoch 24/30\n",
      " - 6s - loss: 1.2070 - acc: 0.3288 - val_loss: 1.1227 - val_acc: 0.3488\n",
      "\n",
      "Epoch 25/30\n",
      " - 6s - loss: 1.2045 - acc: 0.3251 - val_loss: 1.1198 - val_acc: 0.3505\n",
      "\n",
      "Epoch 26/30\n",
      " - 7s - loss: 1.1979 - acc: 0.3301 - val_loss: 1.1239 - val_acc: 0.3505\n",
      "\n",
      "Epoch 27/30\n",
      " - 7s - loss: 1.2042 - acc: 0.3230 - val_loss: 1.1194 - val_acc: 0.3505\n",
      "\n",
      "Epoch 28/30\n",
      " - 7s - loss: 1.2089 - acc: 0.3199 - val_loss: 1.1185 - val_acc: 0.3488\n",
      "\n",
      "Epoch 29/30\n",
      " - 7s - loss: 1.2113 - acc: 0.3345 - val_loss: 1.1165 - val_acc: 0.3488\n",
      "\n",
      "Epoch 30/30\n",
      " - 6s - loss: 1.2038 - acc: 0.3200 - val_loss: 1.1179 - val_acc: 0.3505\n",
      "\n",
      "Test accuracy:\n",
      "0.3505259586019681\n",
      " 83%|████████▎ | 25/30 [46:33<17:26, 209.23s/it, best loss: -0.5310485239226331]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 12s - loss: 1.6809 - acc: 0.3428 - val_loss: 1.5830 - val_acc: 0.3502\n",
      "\n",
      "Epoch 2/30\n",
      " - 5s - loss: 1.5096 - acc: 0.3519 - val_loss: 1.4349 - val_acc: 0.3498\n",
      "\n",
      "Epoch 3/30\n",
      " - 5s - loss: 1.3852 - acc: 0.3519 - val_loss: 1.3184 - val_acc: 0.3502\n",
      "\n",
      "Epoch 4/30\n",
      " - 5s - loss: 1.2940 - acc: 0.3509 - val_loss: 1.2462 - val_acc: 0.3505\n",
      "\n",
      "Epoch 5/30\n",
      " - 5s - loss: 1.2485 - acc: 0.3490 - val_loss: 1.2043 - val_acc: 0.3505\n",
      "\n",
      "Epoch 6/30\n",
      " - 5s - loss: 1.2046 - acc: 0.3532 - val_loss: 1.1697 - val_acc: 0.3502\n",
      "\n",
      "Epoch 7/30\n",
      " - 5s - loss: 1.2055 - acc: 0.3483 - val_loss: 1.1622 - val_acc: 0.3505\n",
      "\n",
      "Epoch 8/30\n",
      " - 5s - loss: 1.1711 - acc: 0.3524 - val_loss: 1.1420 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 5s - loss: 1.1765 - acc: 0.3516 - val_loss: 1.1360 - val_acc: 0.3502\n",
      "\n",
      "Epoch 10/30\n",
      " - 5s - loss: 1.1658 - acc: 0.3527 - val_loss: 1.1767 - val_acc: 0.3441\n",
      "\n",
      "Epoch 11/30\n",
      " - 5s - loss: 1.1717 - acc: 0.3494 - val_loss: 1.1218 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 5s - loss: 1.1535 - acc: 0.3523 - val_loss: 1.1153 - val_acc: 0.3505\n",
      "\n",
      "Epoch 13/30\n",
      " - 5s - loss: 1.1517 - acc: 0.3520 - val_loss: 1.3935 - val_acc: 0.3444\n",
      "\n",
      "Epoch 14/30\n",
      " - 5s - loss: 1.1506 - acc: 0.3523 - val_loss: 1.1110 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 5s - loss: 1.1659 - acc: 0.3531 - val_loss: 1.1085 - val_acc: 0.3505\n",
      "\n",
      "Epoch 16/30\n",
      " - 5s - loss: 1.1473 - acc: 0.3501 - val_loss: 1.1123 - val_acc: 0.3505\n",
      "\n",
      "Epoch 17/30\n",
      " - 5s - loss: 1.1453 - acc: 0.3531 - val_loss: 1.1512 - val_acc: 0.3526\n",
      "\n",
      "Epoch 18/30\n",
      " - 5s - loss: 1.1414 - acc: 0.3515 - val_loss: 1.1166 - val_acc: 0.3509\n",
      "\n",
      "Epoch 19/30\n",
      " - 5s - loss: 1.1347 - acc: 0.3528 - val_loss: 1.1116 - val_acc: 0.3505\n",
      "\n",
      "Epoch 20/30\n",
      " - 5s - loss: 1.1445 - acc: 0.3527 - val_loss: 1.1186 - val_acc: 0.3529\n",
      "\n",
      "Epoch 21/30\n",
      " - 5s - loss: 1.1356 - acc: 0.3570 - val_loss: 1.1117 - val_acc: 0.3661\n",
      "\n",
      "Epoch 22/30\n",
      " - 5s - loss: 1.1396 - acc: 0.3818 - val_loss: 1.1057 - val_acc: 0.5134\n",
      "\n",
      "Epoch 23/30\n",
      " - 5s - loss: 1.1387 - acc: 0.3836 - val_loss: 1.1002 - val_acc: 0.3485\n",
      "\n",
      "Epoch 24/30\n",
      " - 5s - loss: 1.1147 - acc: 0.3531 - val_loss: 1.0682 - val_acc: 0.5059\n",
      "\n",
      "Epoch 25/30\n",
      " - 5s - loss: 1.0712 - acc: 0.5143 - val_loss: 1.0436 - val_acc: 0.5008\n",
      "\n",
      "Epoch 26/30\n",
      " - 5s - loss: 1.0340 - acc: 0.5086 - val_loss: 1.0119 - val_acc: 0.4920\n",
      "\n",
      "Epoch 27/30\n",
      " - 5s - loss: 0.9930 - acc: 0.5180 - val_loss: 0.9724 - val_acc: 0.5032\n",
      "\n",
      "Epoch 28/30\n",
      " - 5s - loss: 0.9468 - acc: 0.5928 - val_loss: 0.8974 - val_acc: 0.6732\n",
      "\n",
      "Epoch 29/30\n",
      " - 5s - loss: 0.8722 - acc: 0.6600 - val_loss: 0.8242 - val_acc: 0.6868\n",
      "\n",
      "Epoch 30/30\n",
      " - 5s - loss: 0.7886 - acc: 0.7035 - val_loss: 0.7468 - val_acc: 0.7611\n",
      "\n",
      "Test accuracy:\n",
      "0.7611129962673906\n",
      " 87%|████████▋ | 26/30 [49:06<12:50, 192.59s/it, best loss: -0.7611129962673906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 12s - loss: 1.6878 - acc: 0.3326 - val_loss: 1.5968 - val_acc: 0.3502\n",
      "\n",
      "Epoch 2/30\n",
      " - 5s - loss: 1.5148 - acc: 0.3523 - val_loss: 1.4437 - val_acc: 0.3502\n",
      "\n",
      "Epoch 3/30\n",
      " - 5s - loss: 1.3827 - acc: 0.3572 - val_loss: 1.3185 - val_acc: 0.3505\n",
      "\n",
      "Epoch 4/30\n",
      " - 5s - loss: 1.2853 - acc: 0.3560 - val_loss: 1.2453 - val_acc: 0.3502\n",
      "\n",
      "Epoch 5/30\n",
      " - 5s - loss: 1.2248 - acc: 0.3564 - val_loss: 1.1963 - val_acc: 0.3505\n",
      "\n",
      "Epoch 6/30\n",
      " - 5s - loss: 1.1891 - acc: 0.3562 - val_loss: 1.1686 - val_acc: 0.3505\n",
      "\n",
      "Epoch 7/30\n",
      " - 5s - loss: 1.1729 - acc: 0.3562 - val_loss: 1.1516 - val_acc: 0.3505\n",
      "\n",
      "Epoch 8/30\n",
      " - 5s - loss: 1.1555 - acc: 0.3557 - val_loss: 1.1277 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 5s - loss: 1.1468 - acc: 0.3572 - val_loss: 1.1340 - val_acc: 0.3505\n",
      "\n",
      "Epoch 10/30\n",
      " - 5s - loss: 1.1334 - acc: 0.3579 - val_loss: 1.1279 - val_acc: 0.3505\n",
      "\n",
      "Epoch 11/30\n",
      " - 5s - loss: 1.1396 - acc: 0.3558 - val_loss: 1.1110 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 5s - loss: 1.1379 - acc: 0.3546 - val_loss: 1.1186 - val_acc: 0.3509\n",
      "\n",
      "Epoch 13/30\n",
      " - 5s - loss: 1.1840 - acc: 0.3557 - val_loss: 1.1746 - val_acc: 0.3414\n",
      "\n",
      "Epoch 14/30\n",
      " - 5s - loss: 1.1602 - acc: 0.3553 - val_loss: 1.1120 - val_acc: 0.3505\n",
      "\n",
      "Epoch 15/30\n",
      " - 5s - loss: 1.1280 - acc: 0.3579 - val_loss: 1.1096 - val_acc: 0.3505\n",
      "\n",
      "Epoch 16/30\n",
      " - 5s - loss: 1.1296 - acc: 0.3565 - val_loss: 1.1125 - val_acc: 0.3505\n",
      "\n",
      "Epoch 17/30\n",
      " - 5s - loss: 1.1279 - acc: 0.3572 - val_loss: 1.1342 - val_acc: 0.3509\n",
      "\n",
      "Epoch 18/30\n",
      " - 5s - loss: 1.1309 - acc: 0.3570 - val_loss: 1.1070 - val_acc: 0.3505\n",
      "\n",
      "Epoch 19/30\n",
      " - 5s - loss: 1.1226 - acc: 0.3562 - val_loss: 1.1165 - val_acc: 0.3512\n",
      "\n",
      "Epoch 20/30\n",
      " - 5s - loss: 1.1469 - acc: 0.3526 - val_loss: 1.1036 - val_acc: 0.3505\n",
      "\n",
      "Epoch 21/30\n",
      " - 5s - loss: 1.1193 - acc: 0.3568 - val_loss: 1.1076 - val_acc: 0.3509\n",
      "\n",
      "Epoch 22/30\n",
      " - 5s - loss: 1.1240 - acc: 0.3573 - val_loss: 1.1014 - val_acc: 0.3509\n",
      "\n",
      "Epoch 23/30\n",
      " - 5s - loss: 1.1286 - acc: 0.3557 - val_loss: 1.1144 - val_acc: 0.3519\n",
      "\n",
      "Epoch 24/30\n",
      " - 5s - loss: 1.1256 - acc: 0.3555 - val_loss: 1.1187 - val_acc: 0.3512\n",
      "\n",
      "Epoch 25/30\n",
      " - 5s - loss: 1.1316 - acc: 0.3570 - val_loss: 1.0985 - val_acc: 0.3505\n",
      "\n",
      "Epoch 26/30\n",
      " - 5s - loss: 1.1230 - acc: 0.3765 - val_loss: 1.0851 - val_acc: 0.4757\n",
      "\n",
      "Epoch 27/30\n",
      " - 5s - loss: 1.1091 - acc: 0.3995 - val_loss: 1.0721 - val_acc: 0.5175\n",
      "\n",
      "Epoch 28/30\n",
      " - 5s - loss: 1.0584 - acc: 0.5333 - val_loss: 0.9826 - val_acc: 0.5219\n",
      "\n",
      "Epoch 29/30\n",
      " - 5s - loss: 0.9461 - acc: 0.6130 - val_loss: 0.8788 - val_acc: 0.6373\n",
      "\n",
      "Epoch 30/30\n",
      " - 5s - loss: 0.8072 - acc: 0.6666 - val_loss: 0.7489 - val_acc: 0.6559\n",
      "\n",
      "Test accuracy:\n",
      "0.6559212758737699\n",
      " 90%|█████████ | 27/30 [51:42<09:04, 181.62s/it, best loss: -0.7611129962673906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 13s - loss: 1.7477 - acc: 0.2193 - val_loss: 1.7952 - val_acc: 0.1805\n",
      "\n",
      "Epoch 2/30\n",
      " - 5s - loss: 1.7879 - acc: 0.1893 - val_loss: 1.7901 - val_acc: 0.1805\n",
      "\n",
      "Epoch 3/30\n",
      " - 5s - loss: 1.7870 - acc: 0.1882 - val_loss: 1.7923 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1882 - val_loss: 1.7984 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 5s - loss: 1.7865 - acc: 0.1877 - val_loss: 1.7904 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1865 - val_loss: 1.8058 - val_acc: 0.1805\n",
      "\n",
      "Epoch 7/30\n",
      " - 5s - loss: 1.7874 - acc: 0.1838 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 8/30\n",
      " - 5s - loss: 1.7862 - acc: 0.1839 - val_loss: 1.7892 - val_acc: 0.1805\n",
      "\n",
      "Epoch 9/30\n",
      " - 5s - loss: 1.7874 - acc: 0.1854 - val_loss: 1.7889 - val_acc: 0.1805\n",
      "\n",
      "Epoch 10/30\n",
      " - 5s - loss: 1.7862 - acc: 0.1907 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 11/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1880 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 12/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1877 - val_loss: 1.7909 - val_acc: 0.1805\n",
      "\n",
      "Epoch 13/30\n",
      " - 5s - loss: 1.7871 - acc: 0.1878 - val_loss: 1.7897 - val_acc: 0.1805\n",
      "\n",
      "Epoch 14/30\n",
      " - 5s - loss: 1.7860 - acc: 0.1878 - val_loss: 1.7894 - val_acc: 0.1805\n",
      "\n",
      "Epoch 15/30\n",
      " - 5s - loss: 1.7864 - acc: 0.1863 - val_loss: 1.7909 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1896 - val_loss: 1.7943 - val_acc: 0.1805\n",
      "\n",
      "Epoch 17/30\n",
      " - 5s - loss: 1.7865 - acc: 0.1840 - val_loss: 1.7906 - val_acc: 0.1805\n",
      "\n",
      "Epoch 18/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1877 - val_loss: 1.7906 - val_acc: 0.1805\n",
      "\n",
      "Epoch 19/30\n",
      " - 5s - loss: 1.7867 - acc: 0.1877 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 20/30\n",
      " - 5s - loss: 1.7863 - acc: 0.1865 - val_loss: 1.7901 - val_acc: 0.1805\n",
      "\n",
      "Epoch 21/30\n",
      " - 5s - loss: 1.7872 - acc: 0.1839 - val_loss: 1.7893 - val_acc: 0.1805\n",
      "\n",
      "Epoch 22/30\n",
      " - 5s - loss: 1.7866 - acc: 0.1872 - val_loss: 1.7923 - val_acc: 0.1805\n",
      "\n",
      "Epoch 23/30\n",
      " - 5s - loss: 1.7853 - acc: 0.1878 - val_loss: 1.7890 - val_acc: 0.1805\n",
      "\n",
      "Epoch 24/30\n",
      " - 5s - loss: 1.7865 - acc: 0.1873 - val_loss: 1.7991 - val_acc: 0.1805\n",
      "\n",
      "Epoch 25/30\n",
      " - 5s - loss: 1.7872 - acc: 0.1878 - val_loss: 1.7898 - val_acc: 0.1805\n",
      "\n",
      "Epoch 26/30\n",
      " - 5s - loss: 1.7868 - acc: 0.1891 - val_loss: 1.7937 - val_acc: 0.1805\n",
      "\n",
      "Epoch 27/30\n",
      " - 5s - loss: 1.7875 - acc: 0.1865 - val_loss: 1.7893 - val_acc: 0.1805\n",
      "\n",
      "Epoch 28/30\n",
      " - 5s - loss: 1.7867 - acc: 0.1865 - val_loss: 1.7893 - val_acc: 0.1805\n",
      "\n",
      "Epoch 29/30\n",
      " - 5s - loss: 1.7871 - acc: 0.1907 - val_loss: 1.7905 - val_acc: 0.1805\n",
      "\n",
      "Epoch 30/30\n",
      " - 5s - loss: 1.7864 - acc: 0.1880 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Test accuracy:\n",
      "0.18052256532066507\n",
      " 93%|█████████▎| 28/30 [54:28<05:53, 176.76s/it, best loss: -0.7611129962673906]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 16s - loss: 1.7069 - acc: 0.2862 - val_loss: 1.6296 - val_acc: 0.3414\n",
      "\n",
      "Epoch 2/30\n",
      " - 7s - loss: 1.5716 - acc: 0.2999 - val_loss: 1.4889 - val_acc: 0.3505\n",
      "\n",
      "Epoch 3/30\n",
      " - 7s - loss: 1.4831 - acc: 0.3018 - val_loss: 1.3807 - val_acc: 0.3502\n",
      "\n",
      "Epoch 4/30\n",
      " - 7s - loss: 1.4218 - acc: 0.3026 - val_loss: 1.3004 - val_acc: 0.3505\n",
      "\n",
      "Epoch 5/30\n",
      " - 7s - loss: 1.3880 - acc: 0.3010 - val_loss: 1.2438 - val_acc: 0.3502\n",
      "\n",
      "Epoch 6/30\n",
      " - 7s - loss: 1.3672 - acc: 0.3005 - val_loss: 1.2104 - val_acc: 0.3502\n",
      "\n",
      "Epoch 7/30\n",
      " - 7s - loss: 1.3544 - acc: 0.2995 - val_loss: 1.1807 - val_acc: 0.3502\n",
      "\n",
      "Epoch 8/30\n",
      " - 7s - loss: 1.3408 - acc: 0.3035 - val_loss: 1.1601 - val_acc: 0.3505\n",
      "\n",
      "Epoch 9/30\n",
      " - 7s - loss: 1.3547 - acc: 0.2976 - val_loss: 1.1575 - val_acc: 0.3505\n",
      "\n",
      "Epoch 10/30\n",
      " - 7s - loss: 1.3538 - acc: 0.3029 - val_loss: 1.1382 - val_acc: 0.3505\n",
      "\n",
      "Epoch 11/30\n",
      " - 7s - loss: 1.3360 - acc: 0.3035 - val_loss: 1.1496 - val_acc: 0.3505\n",
      "\n",
      "Epoch 12/30\n",
      " - 7s - loss: 1.3308 - acc: 0.3054 - val_loss: 1.1399 - val_acc: 0.3509\n",
      "\n",
      "Epoch 13/30\n",
      " - 7s - loss: 1.3113 - acc: 0.3487 - val_loss: 1.1254 - val_acc: 0.5290\n",
      "\n",
      "Epoch 14/30\n",
      " - 7s - loss: 1.2461 - acc: 0.4510 - val_loss: 1.1830 - val_acc: 0.5304\n",
      "\n",
      "Epoch 15/30\n",
      " - 7s - loss: 1.1592 - acc: 0.4601 - val_loss: 1.0120 - val_acc: 0.5280\n",
      "\n",
      "Epoch 16/30\n",
      " - 7s - loss: 1.0839 - acc: 0.4894 - val_loss: 0.9193 - val_acc: 0.6023\n",
      "\n",
      "Epoch 17/30\n",
      " - 7s - loss: 1.0294 - acc: 0.5147 - val_loss: 0.8668 - val_acc: 0.5931\n",
      "\n",
      "Epoch 18/30\n",
      " - 7s - loss: 0.9717 - acc: 0.5007 - val_loss: 0.7690 - val_acc: 0.6376\n",
      "\n",
      "Epoch 19/30\n",
      " - 7s - loss: 0.9331 - acc: 0.5190 - val_loss: 0.7534 - val_acc: 0.6315\n",
      "\n",
      "Epoch 20/30\n",
      " - 7s - loss: 0.8987 - acc: 0.5256 - val_loss: 0.6775 - val_acc: 0.6569\n",
      "\n",
      "Epoch 21/30\n",
      " - 7s - loss: 0.8661 - acc: 0.5290 - val_loss: 0.6290 - val_acc: 0.6695\n",
      "\n",
      "Epoch 22/30\n",
      " - 7s - loss: 0.8366 - acc: 0.5709 - val_loss: 0.6068 - val_acc: 0.6668\n",
      "\n",
      "Epoch 23/30\n",
      " - 7s - loss: 0.7957 - acc: 0.5807 - val_loss: 0.6247 - val_acc: 0.6630\n",
      "\n",
      "Epoch 24/30\n",
      " - 7s - loss: 0.7699 - acc: 0.5940 - val_loss: 0.5635 - val_acc: 0.6698\n",
      "\n",
      "Epoch 25/30\n",
      " - 8s - loss: 0.7589 - acc: 0.6177 - val_loss: 0.5618 - val_acc: 0.6715\n",
      "\n",
      "Epoch 26/30\n",
      " - 7s - loss: 0.7321 - acc: 0.6287 - val_loss: 0.5433 - val_acc: 0.6719\n",
      "\n",
      "Epoch 27/30\n",
      " - 7s - loss: 0.7226 - acc: 0.6322 - val_loss: 0.5891 - val_acc: 0.6481\n",
      "\n",
      "Epoch 28/30\n",
      " - 7s - loss: 0.6910 - acc: 0.7001 - val_loss: 0.5069 - val_acc: 0.7767\n",
      "\n",
      "Epoch 29/30\n",
      " - 7s - loss: 0.6643 - acc: 0.7174 - val_loss: 0.4734 - val_acc: 0.7706\n",
      "\n",
      "Epoch 30/30\n",
      " - 7s - loss: 0.6251 - acc: 0.7450 - val_loss: 0.4579 - val_acc: 0.7710\n",
      "\n",
      "Test accuracy:\n",
      "0.7709535120461486\n",
      " 97%|█████████▋| 29/30 [58:20<03:13, 193.48s/it, best loss: -0.7709535120461486]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/temp_model.py:207: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 16s - loss: 1.7904 - acc: 0.1891 - val_loss: 1.7888 - val_acc: 0.1822\n",
      "\n",
      "Epoch 2/30\n",
      " - 8s - loss: 1.7860 - acc: 0.1907 - val_loss: 1.7951 - val_acc: 0.1822\n",
      "\n",
      "Epoch 3/30\n",
      " - 8s - loss: 1.7863 - acc: 0.1878 - val_loss: 1.7927 - val_acc: 0.1822\n",
      "\n",
      "Epoch 4/30\n",
      " - 8s - loss: 1.7865 - acc: 0.1907 - val_loss: 1.7905 - val_acc: 0.1805\n",
      "\n",
      "Epoch 5/30\n",
      " - 8s - loss: 1.7872 - acc: 0.1884 - val_loss: 1.7896 - val_acc: 0.1805\n",
      "\n",
      "Epoch 6/30\n",
      " - 8s - loss: 1.7864 - acc: 0.1842 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 7/30\n",
      " - 8s - loss: 1.7867 - acc: 0.1897 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 8/30\n",
      " - 8s - loss: 1.7866 - acc: 0.1914 - val_loss: 1.8026 - val_acc: 0.1822\n",
      "\n",
      "Epoch 9/30\n",
      " - 8s - loss: 1.7876 - acc: 0.1885 - val_loss: 1.7894 - val_acc: 0.1822\n",
      "\n",
      "Epoch 10/30\n",
      " - 8s - loss: 1.7867 - acc: 0.1899 - val_loss: 1.7890 - val_acc: 0.1822\n",
      "\n",
      "Epoch 11/30\n",
      " - 8s - loss: 1.7869 - acc: 0.1903 - val_loss: 1.7909 - val_acc: 0.1822\n",
      "\n",
      "Epoch 12/30\n",
      " - 8s - loss: 1.7869 - acc: 0.1876 - val_loss: 1.7920 - val_acc: 0.1805\n",
      "\n",
      "Epoch 13/30\n",
      " - 8s - loss: 1.7873 - acc: 0.1869 - val_loss: 1.7893 - val_acc: 0.1805\n",
      "\n",
      "Epoch 14/30\n",
      " - 8s - loss: 1.7877 - acc: 0.1869 - val_loss: 1.7891 - val_acc: 0.1805\n",
      "\n",
      "Epoch 15/30\n",
      " - 8s - loss: 1.7864 - acc: 0.1869 - val_loss: 1.7890 - val_acc: 0.1805\n",
      "\n",
      "Epoch 16/30\n",
      " - 8s - loss: 1.7869 - acc: 0.1865 - val_loss: 1.7954 - val_acc: 0.1805\n",
      "\n",
      "Epoch 17/30\n",
      " - 8s - loss: 1.7870 - acc: 0.1903 - val_loss: 1.7909 - val_acc: 0.1805\n",
      "\n",
      "Epoch 18/30\n",
      " - 8s - loss: 1.7872 - acc: 0.1888 - val_loss: 1.7900 - val_acc: 0.1822\n",
      "\n",
      "Epoch 19/30\n",
      " - 8s - loss: 1.7868 - acc: 0.1868 - val_loss: 1.7892 - val_acc: 0.1822\n",
      "\n",
      "Epoch 20/30\n",
      " - 8s - loss: 1.7875 - acc: 0.1908 - val_loss: 1.7910 - val_acc: 0.1822\n",
      "\n",
      "Epoch 21/30\n",
      " - 8s - loss: 1.7870 - acc: 0.1902 - val_loss: 1.7903 - val_acc: 0.1822\n",
      "\n",
      "Epoch 22/30\n",
      " - 8s - loss: 1.7874 - acc: 0.1914 - val_loss: 1.7899 - val_acc: 0.1822\n",
      "\n",
      "Epoch 23/30\n",
      " - 8s - loss: 1.7867 - acc: 0.1919 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 24/30\n",
      " - 8s - loss: 1.7873 - acc: 0.1882 - val_loss: 1.7896 - val_acc: 0.1822\n",
      "\n",
      "Epoch 25/30\n",
      " - 8s - loss: 1.7874 - acc: 0.1914 - val_loss: 1.7901 - val_acc: 0.1822\n",
      "\n",
      "Epoch 26/30\n",
      " - 8s - loss: 1.7869 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 27/30\n",
      " - 8s - loss: 1.7866 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Epoch 28/30\n",
      " - 8s - loss: 1.7867 - acc: 0.1914 - val_loss: 1.7891 - val_acc: 0.1822\n",
      "\n",
      "Epoch 29/30\n",
      " - 8s - loss: 1.7874 - acc: 0.1914 - val_loss: 1.7910 - val_acc: 0.1822\n",
      "\n",
      "Epoch 30/30\n",
      " - 8s - loss: 1.7886 - acc: 0.1914 - val_loss: 1.7893 - val_acc: 0.1822\n",
      "\n",
      "Test accuracy:\n",
      "0.18221920597217509\n",
      "100%|██████████| 30/30 [1:02:31<00:00, 210.77s/it, best loss: -0.7709535120461486]\n",
      "{'Dropout': 0.48516778250229436, 'Dropout_1': 0.6508071526801446, 'Dropout_2': 0.6182800609908878, 'Dropout_3': 0, 'Dropout_4': 0.5542133614648339, 'Dropout_5': 0.5858835743656349, 'Dropout_6': 0.040684796653495364, 'Dropout_7': 0.3341762646649339, 'activation': 0, 'activation_1': 1, 'activation_2': 1, 'activation_3': 0, 'activation_4': 1, 'activation_5': 1, 'activation_6': 0, 'activation_7': 1, 'batch_size': 0, 'choiceval': 2, 'filters': 3, 'filters_1': 3, 'filters_2': 4, 'filters_3': 4, 'filters_4': 2, 'filters_5': 4, 'filters_6': 4, 'kernel_size': 0, 'kernel_size_1': 0, 'kernel_size_2': 3, 'kernel_size_3': 0, 'lr': 3, 'lr_1': 1, 'lr_2': 2, 'pool_size': 1, 'pool_size_1': 0, 'pool_size_2': 0, 'pool_size_3': 2}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_105 (Conv1D)          (None, 128, 512)          9728      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_57 (MaxPooling (None, 42, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_189 (Dropout)        (None, 42, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 42, 512)           524800    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_58 (MaxPooling (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_57 (Batc (None, 21, 512)           2048      \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 21, 1024)          2622464   \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 21, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 21504)             0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 256)               5505280   \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "dropout_193 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_194 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 6)                 6150      \n",
      "=================================================================\n",
      "Total params: 9,987,334\n",
      "Trainable params: 9,984,262\n",
      "Non-trainable params: 3,072\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  best_run, best_model = optim.minimize(model=model,\n",
    "                                        data=data,\n",
    "                                        algo=tpe.suggest,\n",
    "                                        max_evals=30,\n",
    "                                        notebook_name='HAR_LSTM_OPT',\n",
    "                                        trials=Trials())\n",
    "  \n",
    "  print(best_run)\n",
    "  print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S5WL0osJWoA"
   },
   "source": [
    "# 1. CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 797
    },
    "colab_type": "code",
    "id": "-Wx8VIEGG4pb",
    "outputId": "37c813f5-006f-4b9d-b643-24a165ea2c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_197 (Conv1D)          (None, 128, 1024)         37888     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_103 (MaxPoolin (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_198 (Conv1D)          (None, 64, 256)           524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_101 (Bat (None, 21, 256)           1024      \n",
      "_________________________________________________________________\n",
      "conv1d_199 (Conv1D)          (None, 21, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_102 (Bat (None, 21, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_200 (Conv1D)          (None, 21, 512)           1049088   \n",
      "_________________________________________________________________\n",
      "dropout_341 (Dropout)        (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_55 (Flatten)         (None, 10752)             0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 1024)              11011072  \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_342 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_343 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_344 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_218 (Dense)            (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 15,284,934\n",
      "Trainable params: 15,282,374\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "timesteps = 128\n",
    "input_dim = 9\n",
    "n_classes = 6\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(Conv1D(filters = 1024, kernel_size=4,\n",
    "                 activation='relu',padding = 'same', input_shape = (timesteps,input_dim)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "# model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Conv1D(filters = 256, kernel_size=2,\n",
    "                 activation='sigmoid',padding = 'same'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Conv1D(filters = 1024, kernel_size=4,\n",
    "             activation='sigmoid',padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(1))\n",
    "\n",
    "model.add(Conv1D(filters = 512, kernel_size=2,\n",
    "             activation='sigmoid',padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "# model.add(Dropout(0))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# adam = keras.optimizers.Adam(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "rmsprop = keras.optimizers.RMSprop(lr=10**-4)\n",
    "# sgd = keras.optimizers.SGD(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=rmsprop)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZZoeenJiH3Cm",
    "outputId": "c6c87757-c07a-475c-f2b7-ae980780b879"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/26\n",
      " - 10s - loss: 0.0928 - acc: 0.9714 - val_loss: 0.4290 - val_acc: 0.9325\n",
      "Epoch 2/26\n",
      " - 10s - loss: 0.0944 - acc: 0.9712 - val_loss: 0.3777 - val_acc: 0.9389\n",
      "Epoch 3/26\n",
      " - 9s - loss: 0.0915 - acc: 0.9724 - val_loss: 0.4942 - val_acc: 0.9298\n",
      "Epoch 4/26\n",
      " - 9s - loss: 0.0844 - acc: 0.9748 - val_loss: 0.3505 - val_acc: 0.9433\n",
      "Epoch 5/26\n",
      " - 9s - loss: 0.0827 - acc: 0.9732 - val_loss: 0.6097 - val_acc: 0.9230\n",
      "Epoch 6/26\n",
      " - 10s - loss: 0.0940 - acc: 0.9720 - val_loss: 0.3820 - val_acc: 0.9308\n",
      "Epoch 7/26\n",
      " - 9s - loss: 0.0792 - acc: 0.9743 - val_loss: 0.3900 - val_acc: 0.9260\n",
      "Epoch 8/26\n",
      " - 9s - loss: 0.0889 - acc: 0.9702 - val_loss: 0.3192 - val_acc: 0.9379\n",
      "Epoch 9/26\n",
      " - 9s - loss: 0.0830 - acc: 0.9744 - val_loss: 0.2692 - val_acc: 0.9511\n",
      "Epoch 10/26\n",
      " - 9s - loss: 0.0834 - acc: 0.9742 - val_loss: 0.3892 - val_acc: 0.9365\n",
      "Epoch 11/26\n",
      " - 9s - loss: 0.0763 - acc: 0.9766 - val_loss: 0.4516 - val_acc: 0.9237\n",
      "Epoch 12/26\n",
      " - 9s - loss: 0.0801 - acc: 0.9750 - val_loss: 0.5175 - val_acc: 0.9209\n",
      "Epoch 13/26\n",
      " - 9s - loss: 0.0909 - acc: 0.9733 - val_loss: 0.5476 - val_acc: 0.9226\n",
      "Epoch 14/26\n",
      " - 9s - loss: 0.0728 - acc: 0.9769 - val_loss: 0.3561 - val_acc: 0.9433\n",
      "Epoch 15/26\n",
      " - 9s - loss: 0.0830 - acc: 0.9770 - val_loss: 0.4454 - val_acc: 0.9359\n",
      "Epoch 16/26\n",
      " - 9s - loss: 0.0952 - acc: 0.9740 - val_loss: 0.4525 - val_acc: 0.9348\n",
      "Epoch 17/26\n",
      " - 9s - loss: 0.0782 - acc: 0.9752 - val_loss: 0.3286 - val_acc: 0.9484\n",
      "Epoch 18/26\n",
      " - 9s - loss: 0.0754 - acc: 0.9765 - val_loss: 0.4314 - val_acc: 0.9413\n",
      "Epoch 19/26\n",
      " - 9s - loss: 0.0886 - acc: 0.9757 - val_loss: 0.3017 - val_acc: 0.9505\n",
      "Epoch 20/26\n",
      " - 9s - loss: 0.0746 - acc: 0.9757 - val_loss: 0.2720 - val_acc: 0.9545\n",
      "Epoch 21/26\n",
      " - 9s - loss: 0.0790 - acc: 0.9774 - val_loss: 0.3371 - val_acc: 0.9386\n",
      "Epoch 22/26\n",
      " - 9s - loss: 0.0674 - acc: 0.9782 - val_loss: 0.3678 - val_acc: 0.9444\n",
      "Epoch 23/26\n",
      " - 9s - loss: 0.0711 - acc: 0.9771 - val_loss: 0.3218 - val_acc: 0.9559\n",
      "Epoch 24/26\n",
      " - 9s - loss: 0.0725 - acc: 0.9814 - val_loss: 0.4420 - val_acc: 0.9355\n",
      "Epoch 25/26\n",
      " - 9s - loss: 0.0702 - acc: 0.9792 - val_loss: 0.3533 - val_acc: 0.9508\n",
      "Epoch 26/26\n",
      " - 9s - loss: 0.0669 - acc: 0.9774 - val_loss: 0.3569 - val_acc: 0.9393\n",
      "Accuracy: 93.93%\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=16,\n",
    "          nb_epoch=26,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "718zA1lIF7Gp",
    "outputId": "26b2fda6-81f0-42d7-d975-a0d18119d066"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCkoppTw0KCillPL4/w6b+VfLkrZiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy Plot')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss Plot')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "tYCRtMjOYEH0",
    "outputId": "d1051efb-67da-43cf-abd2-3c19f625ac4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 530        0  ...                   0                 7\n",
      "SITTING                  1      443  ...                   0                 7\n",
      "STANDING                 0       78  ...                   0                 2\n",
      "WALKING                  0        0  ...                  26                 3\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 420                 0\n",
      "WALKING_UPSTAIRS         0        0  ...                  13               458\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "A68dZ7TpYF0X",
    "outputId": "e5d740b8-0811-40b8-aa44-d9580ad9c13c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 249us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "EuBUI_YFYO0w",
    "outputId": "be7a0cc0-a9f8-4837-9ba6-0f390a0c68c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3568655761598743, 0.9392602646759416]"
      ]
     },
     "execution_count": 94,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Fnk066dJoSM"
   },
   "source": [
    "# 2. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 832
    },
    "colab_type": "code",
    "id": "mxB3Blv4F8T7",
    "outputId": "6feb5828-3c9f-4351-c269-20586c3a9d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_201 (Conv1D)          (None, 128, 1024)         37888     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 64, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_202 (Conv1D)          (None, 64, 1024)          4195328   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_103 (Bat (None, 21, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dropout_345 (Dropout)        (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_203 (Conv1D)          (None, 21, 1024)          4195328   \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 21, 1024)          4096      \n",
      "_________________________________________________________________\n",
      "dropout_346 (Dropout)        (None, 21, 1024)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_204 (Conv1D)          (None, 21, 512)           1049088   \n",
      "_________________________________________________________________\n",
      "dropout_347 (Dropout)        (None, 21, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_56 (Flatten)         (None, 10752)             0         \n",
      "_________________________________________________________________\n",
      "dense_219 (Dense)            (None, 1024)              11011072  \n",
      "_________________________________________________________________\n",
      "dense_220 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_348 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_221 (Dense)            (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_349 (Dropout)        (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_222 (Dense)            (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout_350 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_223 (Dense)            (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 22,662,086\n",
      "Trainable params: 22,657,990\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import LSTM, Conv1D, TimeDistributed, MaxPooling1D\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "    \n",
    "timesteps = 128\n",
    "input_dim = 9\n",
    "n_classes = 6\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(Conv1D(filters = 1024, kernel_size=4,\n",
    "                 activation='relu',padding = 'same', input_shape = (timesteps,input_dim)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "# model.add(Dropout(0.75))\n",
    "\n",
    "model.add(Conv1D(filters = 1024, kernel_size=4,\n",
    "                 activation='sigmoid',padding = 'same'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Conv1D(filters = 1024, kernel_size=4,\n",
    "             activation='sigmoid',padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(1))\n",
    "\n",
    "model.add(Conv1D(filters = 512, kernel_size=2,\n",
    "             activation='sigmoid',padding = 'same'))\n",
    "#         model.add(MaxPooling1D(pool_size={{choice([1,2,3])}}))\n",
    "# model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "# model.add(Dropout(0))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1024, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dropout(0.10))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# adam = keras.optimizers.Adam(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "rmsprop = keras.optimizers.RMSprop(lr=10**-4)\n",
    "# sgd = keras.optimizers.SGD(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=rmsprop)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CkTTqiCBGFGe",
    "outputId": "41ccbcf8-0de2-456b-a62f-7d4a7f6f3cb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/26\n",
      " - 16s - loss: 0.1008 - acc: 0.9650 - val_loss: 0.3108 - val_acc: 0.9372\n",
      "Epoch 2/26\n",
      " - 16s - loss: 0.1044 - acc: 0.9621 - val_loss: 0.3387 - val_acc: 0.9335\n",
      "Epoch 3/26\n",
      " - 16s - loss: 0.0941 - acc: 0.9686 - val_loss: 0.4604 - val_acc: 0.9216\n",
      "Epoch 4/26\n",
      " - 16s - loss: 0.1002 - acc: 0.9656 - val_loss: 0.3405 - val_acc: 0.9372\n",
      "Epoch 5/26\n",
      " - 16s - loss: 0.1028 - acc: 0.9687 - val_loss: 0.3489 - val_acc: 0.9328\n",
      "Epoch 6/26\n",
      " - 16s - loss: 0.1010 - acc: 0.9669 - val_loss: 0.3063 - val_acc: 0.9427\n",
      "Epoch 7/26\n",
      " - 16s - loss: 0.0983 - acc: 0.9676 - val_loss: 0.3478 - val_acc: 0.9352\n",
      "Epoch 8/26\n",
      " - 16s - loss: 0.0996 - acc: 0.9671 - val_loss: 0.3539 - val_acc: 0.9301\n",
      "Epoch 9/26\n",
      " - 16s - loss: 0.0977 - acc: 0.9665 - val_loss: 0.2651 - val_acc: 0.9447\n",
      "Epoch 10/26\n",
      " - 16s - loss: 0.1045 - acc: 0.9683 - val_loss: 0.3146 - val_acc: 0.9287\n",
      "Epoch 11/26\n",
      " - 16s - loss: 0.0997 - acc: 0.9657 - val_loss: 0.2891 - val_acc: 0.9379\n",
      "Epoch 12/26\n",
      " - 16s - loss: 0.0965 - acc: 0.9684 - val_loss: 0.4609 - val_acc: 0.8911\n",
      "Epoch 13/26\n",
      " - 16s - loss: 0.0910 - acc: 0.9702 - val_loss: 0.3531 - val_acc: 0.9321\n",
      "Epoch 14/26\n",
      " - 16s - loss: 0.0935 - acc: 0.9710 - val_loss: 0.2796 - val_acc: 0.9396\n",
      "Epoch 15/26\n",
      " - 16s - loss: 0.0990 - acc: 0.9687 - val_loss: 0.3046 - val_acc: 0.9423\n",
      "Epoch 16/26\n",
      " - 16s - loss: 0.0897 - acc: 0.9714 - val_loss: 0.2873 - val_acc: 0.9430\n",
      "Epoch 17/26\n",
      " - 16s - loss: 0.0914 - acc: 0.9702 - val_loss: 0.3095 - val_acc: 0.9444\n",
      "Epoch 18/26\n",
      " - 16s - loss: 0.0921 - acc: 0.9701 - val_loss: 0.3670 - val_acc: 0.9355\n",
      "Epoch 19/26\n",
      " - 16s - loss: 0.0861 - acc: 0.9713 - val_loss: 0.3773 - val_acc: 0.9318\n",
      "Epoch 20/26\n",
      " - 16s - loss: 0.0977 - acc: 0.9701 - val_loss: 0.3239 - val_acc: 0.9433\n",
      "Epoch 21/26\n",
      " - 16s - loss: 0.0829 - acc: 0.9750 - val_loss: 0.2959 - val_acc: 0.9359\n",
      "Epoch 22/26\n",
      " - 16s - loss: 0.0893 - acc: 0.9733 - val_loss: 0.3352 - val_acc: 0.9257\n",
      "Epoch 23/26\n",
      " - 16s - loss: 0.0828 - acc: 0.9714 - val_loss: 0.4039 - val_acc: 0.9240\n",
      "Epoch 24/26\n",
      " - 16s - loss: 0.0945 - acc: 0.9721 - val_loss: 0.3307 - val_acc: 0.9399\n",
      "Epoch 25/26\n",
      " - 16s - loss: 0.0855 - acc: 0.9710 - val_loss: 0.3710 - val_acc: 0.9352\n",
      "Epoch 26/26\n",
      " - 16s - loss: 0.0831 - acc: 0.9744 - val_loss: 0.2975 - val_acc: 0.9450\n",
      "Accuracy: 94.50%\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=16,\n",
    "          nb_epoch=26,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "9CzAuGYhMShL",
    "outputId": "145d489f-d7db-4521-bcd4-59fbf24e6a6e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGNMQP8f2GgrVnOqFQYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy Plot')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('Loss Plot')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "LspU4BkXcKWY",
    "outputId": "ed9aaa6e-147c-4328-a1d7-c1fd51d33662"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  5      392  ...                   0                 1\n",
      "STANDING                 0       17  ...                   0                 2\n",
      "WALKING                  0        0  ...                  25                 0\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 411                 9\n",
      "WALKING_UPSTAIRS         0        0  ...                   9               461\n",
      "\n",
      "[6 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_9av_yTlcQR0",
    "outputId": "77dd8db7-3ba5-407d-c44d-95ab908bc88a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 1s 376us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dwFuxHthcXIC",
    "outputId": "6a6f866d-bf44-42f5-a5ee-e1bd890623a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29747371452317173, 0.9450288428910757]"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gk-IP72EUK5l",
    "outputId": "858dead4-fd93-4ac3-b39f-70cf9faa2ca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCTF4Xl6J42r"
   },
   "source": [
    "# 3. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "colab_type": "code",
    "id": "7674HfzpPsQ5",
    "outputId": "99e39b31-8b40-4c55-df34-9161c5fa0f11"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0627 14:07:24.449820 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0627 14:07:24.512139 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0627 14:07:24.520497 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0627 14:07:24.876339 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0627 14:07:24.889142 140227015829376 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0627 14:07:24.891833 140227015829376 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0627 14:07:25.178984 140227015829376 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0627 14:07:25.595649 140227015829376 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0627 14:07:25.640149 140227015829376 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0627 14:07:25.676882 140227015829376 nn_ops.py:4224] Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0627 14:07:25.720807 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0627 14:07:25.731528 140227015829376 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128, 200)          168000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128, 200)          320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128, 200)          320800    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 200)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               13107712  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 14,183,046\n",
      "Trainable params: 14,183,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(200, input_shape=(timesteps, input_dim), return_sequences=True))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.75))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(512, activation='sigmoid'))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# adam = keras.optimizers.Adam(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "rmsprop = keras.optimizers.RMSprop(lr=10**-4)\n",
    "# sgd = keras.optimizers.SGD(lr={{choice([10**-4,10**-3, 10**-2, 10**-1])}})\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=rmsprop)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MGtDJloKP3Rc",
    "outputId": "25c8f165-9dc3-4ce2-f2e8-1af4ef5256e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      " - 397s - loss: 0.1025 - acc: 0.9668 - val_loss: 0.5713 - val_acc: 0.9172\n",
      "Epoch 2/30\n",
      " - 398s - loss: 0.1002 - acc: 0.9686 - val_loss: 0.5611 - val_acc: 0.9111\n",
      "Epoch 3/30\n",
      " - 396s - loss: 0.0986 - acc: 0.9675 - val_loss: 0.5544 - val_acc: 0.9220\n",
      "Epoch 4/30\n",
      " - 395s - loss: 0.1022 - acc: 0.9679 - val_loss: 0.6293 - val_acc: 0.9125\n",
      "Epoch 5/30\n",
      " - 395s - loss: 0.1045 - acc: 0.9661 - val_loss: 0.5276 - val_acc: 0.9209\n",
      "Epoch 6/30\n",
      " - 395s - loss: 0.1008 - acc: 0.9693 - val_loss: 0.5803 - val_acc: 0.9131\n",
      "Epoch 7/30\n",
      " - 395s - loss: 0.0927 - acc: 0.9699 - val_loss: 0.5536 - val_acc: 0.9247\n",
      "Epoch 8/30\n",
      " - 395s - loss: 0.0950 - acc: 0.9689 - val_loss: 0.5690 - val_acc: 0.9267\n",
      "Epoch 9/30\n",
      " - 395s - loss: 0.0974 - acc: 0.9689 - val_loss: 0.5748 - val_acc: 0.9220\n",
      "Epoch 10/30\n",
      " - 396s - loss: 0.1004 - acc: 0.9703 - val_loss: 0.7173 - val_acc: 0.9179\n",
      "Epoch 11/30\n",
      " - 396s - loss: 0.0976 - acc: 0.9714 - val_loss: 0.6087 - val_acc: 0.9192\n",
      "Epoch 12/30\n",
      " - 396s - loss: 0.1009 - acc: 0.9691 - val_loss: 0.5639 - val_acc: 0.9277\n",
      "Epoch 13/30\n",
      " - 398s - loss: 0.0989 - acc: 0.9709 - val_loss: 0.5385 - val_acc: 0.9226\n",
      "Epoch 14/30\n",
      " - 398s - loss: 0.1086 - acc: 0.9703 - val_loss: 0.5968 - val_acc: 0.9270\n",
      "Epoch 15/30\n",
      " - 397s - loss: 0.0900 - acc: 0.9728 - val_loss: 0.6980 - val_acc: 0.9087\n",
      "Epoch 16/30\n",
      " - 396s - loss: 0.0977 - acc: 0.9716 - val_loss: 0.7018 - val_acc: 0.9087\n",
      "Epoch 17/30\n",
      " - 395s - loss: 0.0918 - acc: 0.9710 - val_loss: 0.6673 - val_acc: 0.9206\n",
      "Epoch 18/30\n",
      " - 396s - loss: 0.1053 - acc: 0.9693 - val_loss: 0.6565 - val_acc: 0.9220\n",
      "Epoch 19/30\n",
      " - 396s - loss: 0.0966 - acc: 0.9691 - val_loss: 0.5018 - val_acc: 0.9267\n",
      "Epoch 20/30\n",
      " - 397s - loss: 0.0989 - acc: 0.9712 - val_loss: 0.5708 - val_acc: 0.9223\n",
      "Epoch 21/30\n",
      " - 397s - loss: 0.0933 - acc: 0.9743 - val_loss: 0.6162 - val_acc: 0.9152\n",
      "Epoch 22/30\n",
      " - 397s - loss: 0.1001 - acc: 0.9724 - val_loss: 0.5622 - val_acc: 0.9325\n",
      "Epoch 23/30\n",
      " - 395s - loss: 0.0863 - acc: 0.9758 - val_loss: 0.6157 - val_acc: 0.9097\n",
      "Epoch 24/30\n",
      " - 397s - loss: 0.0892 - acc: 0.9737 - val_loss: 0.6325 - val_acc: 0.9237\n",
      "Epoch 25/30\n",
      " - 397s - loss: 0.0969 - acc: 0.9758 - val_loss: 0.5626 - val_acc: 0.9250\n",
      "Epoch 26/30\n",
      " - 399s - loss: 0.0926 - acc: 0.9747 - val_loss: 0.7069 - val_acc: 0.9216\n",
      "Epoch 27/30\n",
      " - 397s - loss: 0.1002 - acc: 0.9752 - val_loss: 0.6214 - val_acc: 0.9230\n",
      "Epoch 28/30\n",
      " - 396s - loss: 0.1004 - acc: 0.9755 - val_loss: 0.6882 - val_acc: 0.9213\n",
      "Epoch 29/30\n",
      " - 398s - loss: 0.0993 - acc: 0.9731 - val_loss: 0.6568 - val_acc: 0.9220\n",
      "Epoch 30/30\n",
      " - 396s - loss: 0.0890 - acc: 0.9766 - val_loss: 0.6553 - val_acc: 0.9162\n",
      "Accuracy: 91.62%\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "          batch_size=16,\n",
    "          nb_epoch=30,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, Y_test))\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "3rAiQRYMIPyA",
    "outputId": "b162dbad-251b-4395-ac18-a0754394a612"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+---------------+\n",
      "|   Model    | Train Accuracy | Test Accuracy |\n",
      "+------------+----------------+---------------+\n",
      "| Conv model |     0.9774     |     0.9393    |\n",
      "| Conv model |     0.9744     |     0.945     |\n",
      "| LSTM model |     0.9766     |     0.9162    |\n",
      "+------------+----------------+---------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Model\", \"Train Accuracy\", \"Test Accuracy\"]\n",
    "\n",
    "x.add_row([\"Conv model\", 0.9774, 0.9393])\n",
    "x.add_row([\"Conv model\", 0.9744, 0.9450])\n",
    "x.add_row([\"LSTM model\", 0.9766, 0.9162])\n",
    "\n",
    "\n",
    "print(x.get_string(title=\"Multiple LSTM Model\"),'\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HAR_LSTM_OPT.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
